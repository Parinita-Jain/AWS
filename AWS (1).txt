5 essential characteristics of cloud computing--
1. On demand self-service
2. Broad network access
3. Resource pooling
4. Rapid elasticity
5. Measured service

networking-storage-servers-virtualization-os-middleware
-runtime-data-applications

IAAS-- u manage end-to-end 
PAAS-- os-middleware-runtime-data-applications u manage
SAAS-- data-applications

1960s- IBM and DEC provide timesharing
1972- IBM develops first VM
1977- Cloud symbol used
1991- www launched
1997- word cloud coined by Ramesh Chellappa
1999- VMWare saleforce incorporated
2002- AWS launched
2006- Hadoop released
2008- Google app engine released
2010- Microsoft azure released
2017- Pay per second billing by AWS and GCP
2019- CDN Market Booms

3 types of clouds--
Public 
Private
Hybrid

Public - share with others, cheaper,pay as u go, no maintenance
Infrastructure owned and operated by cloud service provider,
pay only for what u use,
reduce complexity,
quicker setup

Private- Upfront funding, maintenance charges , privacy, flexibility
self owned data centers and software for private use,
dedicated,
secure,
regulation complaint,
highly customizable 

Hybrid- rental car-- higher fee, privacy,flexibility,no maintenance
mix and match public and private cloud based on requirements,
improved security
minimal security risk
high flexibility

drawbacks--
public-- lesser security,limited flexibility
private-- higher investment,maintenance cost
hybrid-- expensive in long run, coplexity is high.

AWS --

IAM- identity Access mgmt
Identity Authentication and access mgmt. It is a web service
that securely helps control access to AWS resources.
U use IAM to control who is authorized (signed in)
and authorized (has pernission) to use resources.

1. sign-up for aws--
verify
root user pwd-- confirm pwd
personal

on successful registration u get aws mgmt console.

sign in- root user

Now a company shares I AM account with employees
and not root user account. IAM is a framework or
policies.

root user-- create many user passwords called as I AM user,
give permissions to different user,

Command line (CIL)and GUI, MFA

IAM user and ploicies--
root user can create 5000 IAM user .
can add upto 10 users in 1 login
limited to 300 groups per AWS account.
to every user, 1000 IAM roles can be given under AWS account.
Default limits to managed policies attached 
to an IAM role and IAM user is 10.
1 IAM user can be a member of 10 groups max. We can increase it.
We can assign 2 access keys(max) to an IAM user.


Creating IAM user--


Compute services-- ec2, lightsail,elastic beanstalk,lambda

ec2 instance--
A server is a computer connected to a network of other workstations called 'clients'. Client computers request information from the server over the network. Servers tend to have more storage, memory and processing power than a normal workstation.
A virtual server in the AWS Cloud is called as an Instance. With Amazon EC2, you can set up and configure the operating system and applications that run on your instance.

You can use Amazon EC2 to launch as many or as few virtual servers as you need, configure security and networking, and manage storage. Amazon EC2 enables you to scale up or down to handle changes in requirements or spikes in popularity, reducing your need to forecast traffic.

Amazon EC2 Auto Scaling helps you maintain application availability and allows you to automatically add or remove EC2 instances according to conditions you define.

You can use the fleet management features of EC2 Auto Scaling to maintain the health and availability of your fleet.

Dynamic scaling responds to changing demand and predictive scaling automatically schedules the right number of EC2 instances based on predicted demand.

Groups
Groups are the logical groups which contain the collection of EC2 instances with similar characteristics for scaling and management purpose.

Launch Configuration
The launch configuration is a template used by auto scaling group to launch EC2 instances. You can specify the Amazon Machine Image (AMI), instances type, key pair, and security groups etc.. while creating the launch configuration. You can also modify the launch configuration after creation. Launch configuration can be used for multiple auto scaling groups.

Scale based on demand:- This is the most advanced scaling model, resources scales by using a scaling policy. Based on specific parameters you can scale in or scale out your resources. You can create a policy by defining the parameter such as CPU utilization, Memory, Network In and Out etc. For Example, you can dynamically scale your EC2 instances which exceeds the CPU utilization beyond 70%. If CPU utilization crosses this threshold value, the auto scaling launches new instances using the launch configuration. You should specify two scaling policies, one for scaling In (terminating instances) and one for scaling out (launching instances).





instances--launch instance--
name - windows_machine
AMI- template that contains software configurations required to launch an instance.--- windows---microsoft windows server 2022 base
instance type- t2.micro
key pair login-- create new key pair--windows_machine -- .pem --save 

Network sttings--
create security group-- allow rdp traffic from-- anywhere-- 

configure storage--
30 gb we get

summary--
no. of instances--1

--launch instance


AWS storage service-- Elastic block storage,
Elastic File System, Simple Storage Service,
S3 glacier, Storage Gateway


Elastic load balancing-- Classic load balancer, application load balancer, network load balancer

Databases-- Amazon Aurora, Elastic Cache,
Dynamo DB, Relational Database Service (RDS),
Amazon Neptune

AWS global infrastructure--
Route53, Route53 as registrar, Record sets and routing policies

Infrastructure mgmt--
Cloud formation, Elastic beanstalk, Code deploy, code commit, code build, code pipeline, code artifact
Systems manager (SSM)

Application Integration services--
API Gateway, EventBridge, SNS, SQSx, Amazon MQ, AWS step functions

Cloud monitoring--
Cloud watch, cloud trial, cloud guru

VPC and networking--
VPC, Security group and NACL

Security--
Encryption with KMS, AWS certification manager, DDoS protection- WAF & Shield,
Artifact and GuardDuty , Config, Security hub and root user privileges

Other services--
Elastic Container service(ECS), Elastic Kubernetes service(EkS), cloudfront,
AWS sagemaker, AWS sumerian

Amazon lightsail-- Simple and affordable way to host websites and applications on the internet.
U can think of it as renting land, ready made homes with pre configured packages. this is everything that u need to run a website 
such as web server, database, storage with
fixed predictable monthly bills.
U dont need to be computer expert to use this service.

Elstic beanstalk--
It creates a space on the internet where ur 
website can live.With its help, u can 
make ur website look good. U can add colors,
pictures and text to make it attractive
. In case if something goes wrong, then it knows how to fix it.
Its like having an assistant who knows how to set up and manage ur website. It takes care of the technical side of ur website
so u can focus on what u want to share with the world.

AWS lambda--
Serverless computing service that lets u run ur code without the need to manage servers i.e 
it automatically scales up ur applications based on number of requests or events.

It is triggered by events eg changes to data 
in amazon s3 buckets or updates to dynamo db, http req via API gateway, etc.

Pay per use-- not charged when code is not running.

languages supported like-- node js, python, java, etc

reate a Lambda Function:



You write your code and package it into a deployment package. This package includes your code, any dependencies, and a configuration file.

Upload to Lambda:



You upload your deployment package to AWS Lambda using the AWS Management Console, AWS CLI, or an SDK.

Define Triggers:



You configure what triggers the Lambda function. This could be an HTTP request, changes to a database, modifications to a file in an S3 bucket, etc.

Execution:



When the specified trigger occurs, AWS Lambda executes your function. It automatically scales to handle the load.

Monitoring and Logging:



AWS Lambda provides monitoring and logging through Amazon CloudWatch, allowing you to track the performance and troubleshoot any issues.

Use Cases:

Microservices:



Lambda is well-suited for building microservices. Each function can represent a specific functionality, and they can be orchestrated to build complex applications.

Event-Driven Applications:



If your application responds to events, such as changes in data or user actions, Lambda is a great fit.

Automation:



Lambda can be used for automating tasks, such as image or video processing, file cleanup, or data transformation.

Benefits:

Scalability:



Lambda automatically scales with the number of incoming requests, ensuring that your application can handle varying workloads.

Cost-Efficiency:



You only pay for the compute time your code uses, making it cost-effective, especially for sporadically used applications.

Reduced Operational Overhead:



Since AWS takes care of the underlying infrastructure, you can focus more on your code and less on server management.

In summary, AWS Lambda simplifies the process of deploying and running code, allowing you to focus on building your applications without the need to manage servers. It's a powerful tool for modern, scalable, and cost-effective cloud computing.

Amazon elastic block storage--
EBS is like a disk drive of an instance 
either SSd or HDD. They are attached to an EC2 instance and physically reside in the same availability zone as the EC2 instance.

Amazon Elastic Block Store (Amazon EBS) provides block level storage volumes for use with EC2 instances.
EBS volumes behave like raw, unformatted block devices.
You can mount these volumes as devices on your instances.
EBS volumes that are attached to an instance are exposed as storage volumes that persist independently from the life of the instance.
You can create a file system on top of these volumes, or use them in any way you would use a block device (such as a hard drive). You can dynamically change the configuration of a volume attached to an instance.
 

 
With Amazon EBS, you pay only for what you use.

Snapshots
Snapshots are just like the backups that you activate in your phone, laptop or any devices to make sure that even if you loose your data's by any chance you can retrieve it if you already have a backup of it.
The same process can be done with your Block storage too.

Amazon EBS provides the ability to create snapshots (backups) of any EBS volume and write a copy of the data in the volume to Amazon S3, where it is stored redundantly in multiple Availability Zones.


Amazon elastic file system--

Lets discuss about NFS first and then move on to EFS..


Network File System ( NFS ) is nothing is a distributed file system protocol originally developed by Sun Microsystems (Sun) in 1984, allowing a user on a client computer to access files over a computer network much like local storage is accessed.
AWS EFS supports NFS 4.0 and 4.1, but an important caveat is that NFS on Amazon only works with Linux instances (Amazon provides shared storage for Windows using a different service, Amazon FSx)



Amazon Elastic File System is a cloud storage service provided by Amazon Web Services designed to provide scalable, elastic, concurrent with some restrictions, and encrypted file storage for use with both AWS cloud services and on-premises resources.

Amazon EFS provides scalable file storage for use with Amazon EC2. You can use an EFS file system as a common data source for workloads and applications running on multiple instances.

What is AWS EFS used for?
Amazon EFS provides a simple, serverless, set-and-forget elastic file system. With Amazon EFS, you can create a file system, mount the file system on an Amazon EC2 instance, and then read and write data to and from your file system.
 
Amazon EFS features
Serverless
Highly available and durable
Storage classes and lifecycle management
Security and compliance
Scalable performance
Shared file system with NFS v4.0 and v4.1 support
Performance modes
Containers and serverless file storage

Amazon simple storage service--

Amazon Simple Storage Service (Amazon S3) is a scalable object storage service provided by Amazon Web Services (AWS). It is designed to store and retrieve any amount of data from anywhere on the web. Let's break down the key concepts and features:



Key Concepts:

Object Storage:



Amazon S3 is an object storage service, which means it stores data as objects. Each object consists of data, a key (unique within a bucket), and metadata.

Buckets:



In Amazon S3, data is stored in containers called "buckets." A bucket is like a top-level folder for organizing objects. Each bucket has a globally unique name within AWS.

Objects:



Objects are the basic unit of storage in Amazon S3. They can be anything from a simple text file to a complex set of data. Objects can range in size from a few bytes to terabytes.

Keys:



The key is a unique identifier for an object within a bucket. The combination of the bucket name and the object key forms a unique URL for each object stored in Amazon S3.

Regions:



Amazon S3 is available in different geographical regions. When you create a bucket, you choose the region where the bucket will be stored. This allows you to optimize latency, minimize costs, or address regulatory requirements.

Features and Use Cases:

Durability and Availability:



Amazon S3 is designed for 99.999999999% (11 9's) durability, meaning your data is highly resilient to hardware failures. It also provides 99.99% availability.

Scalability:



S3 automatically scales to handle growing amounts of data. You don't need to worry about capacity planning or performance optimization.

Security:



S3 provides multiple layers of security. You can control access to your buckets and objects using Access Control Lists (ACLs) and bucket policies. It also integrates with AWS Identity and Access Management (IAM) for fine-grained access control.

Versioning:



Amazon S3 supports versioning, allowing you to preserve, retrieve, and restore every version of every object stored in a bucket. This is useful for data recovery and protection against accidental deletions.

Lifecycle Policies:



You can define lifecycle policies to automatically transition objects to different storage classes or delete them when they are no longer needed.

Static Website Hosting:



S3 can be used to host static websites, making it a cost-effective solution for hosting static content like HTML, CSS, and JavaScript files.

Data Transfer Acceleration:



Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files to and from Amazon S3. It utilizes the Amazon CloudFront global content delivery network (CDN) to accelerate transfers.

How It Works:

Create a Bucket:



You start by creating a bucket in a specific AWS region. This is like creating a top-level folder to store your objects.

Upload Objects:



You upload your objects (files, documents, images, etc.) into the buckets. Each object is assigned a unique key.

Access Control:



You can control who has access to your buckets and objects by configuring bucket policies, IAM roles, and Access Control Lists (ACLs).

Retrieve and Use:



Once your data is in Amazon S3, you can retrieve it programmatically or through the AWS Management Console. It can be used for various purposes, such as hosting static websites, storing backups, or serving as a data lake.

Use Cases:

Backup and Restore:



Businesses use S3 to store backup copies of their critical data for disaster recovery.

Data Archiving:



S3 provides a cost-effective solution for archiving infrequently accessed data.

Content Distribution:



It can be used in conjunction with Amazon CloudFront to distribute content globally with low-latency and high transfer speeds.

Big Data Analytics:



Amazon S3 is often used as a data lake for big data analytics. Data can be ingested and analyzed directly from S3.

Static Website Hosting:



S3 is a popular choice for hosting static websites due to its scalability and cost-effectiveness.

In summary, Amazon S3 is a highly durable, scalable, and secure object storage service that allows you to store and retrieve any amount of data from anywhere on the web. It is a foundational service in the AWS ecosystem and is widely used for various storage needs.

amazon glacier--
Amazon S3 Glacier is a part of the Amazon S3 (Simple Storage Service) family and is designed for long-term archival and backup of data at a very low cost. It is a secure, durable, and scalable storage solution for data archiving. Here are the key aspects of Amazon S3 Glacier:



Key Concepts:

Archives:



In Amazon S3 Glacier, data is stored as archives. An archive can be any data object, such as a document, photo, or video, and can range in size from a few bytes to multiple terabytes.

Vaults:



Archives are stored in containers called "vaults." A vault is a logical container for storing archives and helps in organizing and managing your data. Each AWS account can have multiple vaults.

Vault Lock:



S3 Glacier offers a feature called "Vault Lock," which allows you to enforce compliance controls by configuring vault access policies. Once a vault lock policy is in place, it cannot be changed, ensuring that your data is immutable and remains compliant with regulatory requirements.

Job Management:



Operations in S3 Glacier, such as uploading an archive or retrieving data, are managed through jobs. You initiate a job to perform actions like retrieving an archive for download or inventorying the contents of a vault.

Storage Classes:

S3 Glacier:



This is the default storage class and is suitable for most long-term archival use cases. It provides low-cost storage with a standard retrieval time of a few hours.

S3 Glacier Deep Archive:



This is the lowest-cost storage class in Amazon S3 and is designed for the most cost-sensitive archival workloads. Retrieval times are longer compared to S3 Glacier, typically taking 12 hours.

How It Works:

Upload Archives:



You create archives by uploading data to a vault. Each archive is assigned a unique identifier.

Inventory:



You can use the S3 Glacier inventory feature to generate reports about your vault's contents. This helps you keep track of the archives stored in your vaults.

Retrieve Data:



Retrieving data from S3 Glacier involves initiating a retrieval job. Depending on the retrieval option you choose (standard or bulk), it might take several hours to get your data back.

Notifications:



S3 Glacier supports Amazon SNS (Simple Notification Service) to send notifications when certain events occur, such as the completion of an archive retrieval job.

Use Cases:

Data Archiving:



S3 Glacier is ideal for archiving data that is infrequently accessed but needs to be retained for compliance or other reasons.

Backup and Restore:



Businesses use S3 Glacier for long-term backup and restore scenarios where data is rarely accessed but must be preserved.

Compliance:



The immutability features, such as Vault Lock, make S3 Glacier suitable for storing data that must adhere to regulatory compliance requirements.

Benefits:

Low Cost:



S3 Glacier and S3 Glacier Deep Archive are designed to be cost-effective for long-term archival. Deep Archive offers the lowest storage costs.

Durability and Reliability:



Archives stored in S3 Glacier benefit from the same durability and reliability as other Amazon S3 storage classes.

Security and Compliance:



S3 Glacier provides security features like encryption at rest and in transit. Vault Lock helps in meeting compliance requirements.

Scalability:



S3 Glacier can scale to accommodate large amounts of archival data, making it suitable for businesses with varying storage needs.

In summary, Amazon S3 Glacier is a specialized storage service within the Amazon S3 ecosystem, focusing on long-term archival and backup use cases. It provides a cost-effective solution for storing data that is rarely accessed but needs to be retained for extended periods.

Amazon Storage gateway--

Amazon Storage Gateway is a hybrid cloud storage service provided by Amazon Web Services (AWS). It enables on-premises applications to seamlessly integrate with AWS cloud storage services, extending the capacity and capabilities of your on-premises environment. The Storage Gateway service is designed to simplify the process of integrating your on-premises infrastructure with cloud storage.



Here are the key components and features of Amazon Storage Gateway:



Components:

Gateway Appliance:



The Storage Gateway includes a virtual or hardware appliance that you deploy on-premises. This appliance acts as a bridge between your on-premises environment and the AWS Cloud.

Storage Gateway Console:



The AWS Management Console provides a web-based interface for configuring and managing your Storage Gateway. You use the console to create and manage gateways, monitor performance, and configure settings.

Gateway-Cached Volumes:



This configuration stores your primary data locally on-premises, while asynchronously backing up the data to Amazon S3 in the cloud. This is suitable for frequently accessed data that requires low-latency access.

Gateway-Stored Volumes:



In this configuration, your entire dataset is stored on-premises, and the entire dataset is also backed up to Amazon S3. This is suitable for larger datasets that are infrequently accessed.

Gateway-Virtual Tape Library (VTL):



Storage Gateway VTL provides a way to replace your physical tape infrastructure with virtual tapes. It allows you to archive data to virtual tapes in Amazon S3, providing cost-effective long-term storage.

Features:

Seamless Integration:



Storage Gateway integrates with your existing on-premises applications seamlessly. Applications continue to operate as if the data were stored locally while also benefiting from the durability and scalability of AWS cloud storage.

Low Latency Access:



Gateway-Cached Volumes provide low-latency access to frequently accessed data by keeping a copy of the frequently accessed data locally.

Data Deduplication and Compression:



Storage Gateway employs data deduplication and compression to optimize data transfer between your on-premises environment and the AWS Cloud, reducing the amount of data sent over the network.

Snapshot Management:



You can take point-in-time snapshots of your volumes, providing a way to recover data to a specific state. Snapshots are stored in Amazon S3.

Security:



Storage Gateway ensures data security during transfer and at rest. Data can be encrypted in transit using SSL, and at rest using AWS Key Management Service (KMS) for added security.

Use Cases:

Data Migration:



You can use Storage Gateway to migrate on-premises data to the cloud without disrupting your applications.

Backup and Recovery:



Storage Gateway provides a reliable solution for backing up your on-premises data to the cloud, allowing for easier recovery in case of data loss.

Tiered Storage:



It enables you to tier your data, keeping frequently accessed data on-premises for low-latency access and less frequently accessed data in the cloud for cost savings.

Disaster Recovery:



Storage Gateway can be part of your disaster recovery strategy, allowing you to replicate data to the cloud for better resilience.

How It Works:

Deploy Gateway Appliance:



You deploy the Storage Gateway virtual or hardware appliance on-premises.

Configure Gateway:



Using the Storage Gateway Console, you configure the gateway and choose the appropriate storage configuration (Cached Volumes, Stored Volumes, or VTL).

Integrate with Applications:



Your on-premises applications continue to interact with the storage gateway as if it were a local storage device.

Data Transfer:



The gateway manages the transfer of data between your on-premises environment and the designated storage in the AWS Cloud.

Monitor and Manage:



You can monitor the performance, manage snapshots, and configure settings through the Storage Gateway Console.

In summary, Amazon Storage Gateway is a hybrid cloud 
storage service that simplifies the integration of 
on-premises environments with AWS Cloud storage. 
It enables data migration, backup, and 
tiered storage while providing a seamless experience 
for on-premises applications.

Compute services--
ELB-- for distributed computing architecture.
Their role is to distribute incoming network traffic
among cluster of distributed servers so that the workdone by single
computer gets distributed among multiple computers
.
3 types of load balancers are--
1.classic lb-- first-ever load balancer introduced by AWS in the year 2009-- http,https,tcp
How Classic Load Balancers work
1.  Your client makes a request to your application.
2.  The listeners in your load balancer receive requests for the matching protocol and port.
3.  Each listener forwards requests to registered instances which are considered healthy according to the health checks you configure. TCP listeners apply round-robin routing while HTTP/HTTPS listeners apply the least outstanding requests algorithm. You can add or remove instances from your load balancer as needed without disrupting the overall flow of requests to your application.
Note: Classic load balancers cannot route to target groups.



2.application lb-- http,https,websocket

The second type of load balancer that we are going to cover is Application Load Balancer.

Application Load Balancer (v2) is the new generation load balancer that was introduced in 2016.

ALB supports HTTP, HTTPS, Websocket protocol. 



AWS Application Load Balancer (ALB) operates at Layer 7 of the OSI model. At Layer 7, the ELB can inspect application-level content, not just IP and port. This lets it route based on more complex rules than with the Classic Load Balancer.

 

In another example, an ELB at a given IP will receive a request from the client on port 443 (HTTPS). The Application Load Balancer will process the request, not only by receiving port but also by looking at the destination URL.

 

Multiple services can share a single load balancer using path-based routing. In the example given here, the client could request any of the following URLs:

 

Application Load Balancer will be aware of each of these URLs based on patterns set up when configuring the load balancer and can route to different clusters of servers depending on application need. Rules can also be added at a later time as you add new functionality to your stack.

The Application Load Balancer also integrates with EC2 Container Service (ECS) using Service Load Balancing. 
This allows for dynamic mapping of services to ports as specified in the ECS task definition. 
Multiple containers can be targeted on the same EC2 instance, each running different services on different ports. 
The ECS task scheduler will automatically add these tasks to the ALB.


3.network lb-- tcp,tls(secure tcp) and udp
- u can set up internal(private) or external(public) elbs
NLB supports TCP, TLS & UDP.

A Network Load Balancer functions at the fourth layer of the Open Systems Interconnection (OSI) model.

It can handle millions of requests per second. After the load balancer receives a connection request, it selects a target from the target group for the default rule.

It attempts to open a TCP connection to the selected target on the port specified in the listener configuration.

--- Amazon database--

Amazon Aurora is a fully managed relational database engine provided by Amazon Web Services (AWS). 
It is compatible with MySQL and PostgreSQL, offering high performance, scalability, and reliability. 
Aurora is designed to address the challenges of traditional relational databases while providing the benefits of cloud-native architecture.

Key Features:



1. Performance: Aurora is known for its high performance. It provides low-latency read replicas and supports automatic read and write load balancing. This makes it well-suited for applications with demanding performance requirements.



2. Compatibility: Aurora is compatible with MySQL and PostgreSQL, which means you can use familiar tools and libraries with Aurora databases. This makes it easy for developers to migrate existing applications to Aurora.



3. Automatic Scaling: Aurora can automatically scale both read and write capacity to handle varying workloads. This allows your database to adapt to changing demands without manual intervention.



4. Fault-Tolerant and Highly Available: Aurora replicates your database volume six ways across three Availability Zones (AZs). This ensures high availability and durability. In the event of a failure, Aurora can automatically and transparently failover to a replica.



5. Continuous Backups and Point-in-Time Recovery: Aurora automatically takes continuous backups of your database volume. You can also perform point-in-time recovery to restore your database to any second in the past, up to the last five minutes.



6. Global Databases: Aurora Global Databases allow you to replicate your database across multiple regions for low-latency global access. This is useful for building globally distributed applications.



7. Security: Aurora provides security features such as encryption at rest and in transit. It supports Amazon Virtual Private Cloud (VPC) for network isolation, and you can use AWS Identity and Access Management (IAM) for access control.



8. Serverless Aurora (Aurora Serverless): Aurora Serverless is a mode of Aurora that automatically adjusts its capacity based on actual usage. This can be particularly beneficial for applications with unpredictable workloads, as it eliminates the need for manual capacity planning.



How It Works:



1. Database Instances: An Aurora database consists of one primary instance and up to 15 read replicas. The primary instance handles write operations, and read replicas handle read operations.



2. Storage: Aurora uses a distributed and fault-tolerant storage system that automatically divides your database volume into 10GB segments spread across many disks. Each 10GB chunk of your database volume is replicated six ways, across three Availability Zones.



3. Read Replicas: Aurora allows you to create up to 15 read replicas, which can be used to offload read traffic and improve performance. Read replicas are automatically kept up to date with the primary instance.



4. Automatic Failover: In the event of a failure, Aurora can automatically and transparently failover to a read replica, minimizing downtime.



5. Global Databases: You can create a global database that spans multiple AWS regions. This allows you to have low-latency access to your data globally.



Use Cases:



1. High-Performance Applications: Aurora is well-suited for applications with high-performance requirements, such as e-commerce platforms, gaming applications, and financial services.



2. Mission-Critical Workloads: Due to its high availability, durability, and automatic failover capabilities, Aurora is suitable for mission-critical workloads that require minimal downtime.



3. Global Applications: Global Databases in Aurora make it a good choice for applications that need low-latency access to data from multiple geographic regions.



4. Scalable Workloads: Aurora's automatic scaling features make it suitable for workloads with varying demand, allowing the database to scale up or down based on usage.



5. Migration from MySQL or PostgreSQL: If you're using MySQL or PostgreSQL and looking to migrate to a fully managed service with improved performance, Aurora provides compatibility with these database engines.



In summary, Amazon Aurora is a powerful and fully managed relational database service that combines the best features of traditional databases with the benefits of cloud-native architecture. It is designed to deliver high performance, availability, and scalability for a wide range of applications and workloads.

Amazon ElasticCache is a service provided by Amazon Web Services (AWS) that helps make your web applications more responsive by allowing them to retrieve information quickly. Think of it as a tool that helps your applications run faster and smoother.



Now, let's break it down:



1. CACHING FOR SPEED:

   Imagine you have a favorite restaurant where the chef already knows your favorite dish. Every time you go there, your dish is ready in a flash because the chef doesn't have to start from scratch. In a similar way, ElasticCache stores frequently used data so that when your application needs it, it's readily available. This makes your application respond faster, just like getting your favorite dish quickly.



2. TWO POPULAR TYPES:

   ElasticCache supports two popular caching engines - Redis and Memcached. These engines help store and retrieve data lightning fast. It's like having two different recipes (engines) for your favorite dish (data), and you can choose the one that suits your taste (application requirements).



3. SCALABILITY:

   As your application grows, the demand for data also increases. ElasticCache allows you to easily scale up your caching capacity to handle more data and users. It's like expanding your favorite restaurant so that more people can enjoy their favorite dishes without compromising on speed.



4. AUTOMATIC MANAGEMENT:

   Managing a cache might sound complicated, but ElasticCache takes care of the nitty-gritty details for you. It automatically handles tasks like data replication and backups, ensuring that your data is safe and always available. It's like having a trusty assistant in the kitchen who takes care of all the behind-the-scenes work, so you can focus on enjoying your meal (or running your application).



5. INTEGRATION WITH AWS SERVICES:

   ElasticCache seamlessly integrates with other AWS services, making it easy to incorporate into your existing applications. It's like having a versatile ingredient that blends well with different recipes, enhancing the overall flavor of your application.



In summary, Amazon ElasticCache is like a super-efficient kitchen for your web applications. It helps them serve data quickly, scale effortlessly, and ensures that everything runs smoothly behind the scenes. Just like a well-organized kitchen makes your dining experience enjoyable, ElasticCache makes your applications fast and responsive for a better user experience.

Amazon DynamoDB is a fully managed NoSQL database service provided by Amazon Web Services (AWS). It's designed to store and retrieve any amount of data, and it's known for being fast, highly available, and scalable. Let's break it down:



1. DATA STORAGE:

   DynamoDB is like a super organized and efficient filing cabinet for your data. It can store vast amounts of information, and you can easily access and retrieve specific pieces of data whenever you need them. It's perfect for applications that deal with large volumes of data.



2. NO-SQL STRUCTURE:

   Unlike traditional relational databases, DynamoDB follows a NoSQL structure. This means it can handle diverse types of data without a rigid schema. Think of it as a versatile storage system that adapts to different types of information, making it suitable for dynamic and evolving applications.



3. SCALABILITY:

   DynamoDB is built to scale with your needs. As your application grows and requires more storage or faster performance, DynamoDB can easily accommodate. It's like having the ability to add more drawers to your filing cabinet as your paperwork increases, ensuring that you always have enough space.



4. HIGH AVAILABILITY:

   DynamoDB is designed for high availability, meaning your data is accessible and reliable. It automatically replicates your data across multiple locations, making sure that even if one location encounters an issue, your data is still available elsewhere. It's like having copies of your important documents stored in different secure places.



5. AUTOMATIC MANAGEMENT:

   Managing a database can be complex, but DynamoDB simplifies it. It takes care of tasks like hardware provisioning, setup, configuration, and maintenance. It's like having a dedicated assistant who ensures your filing cabinet is always well-maintained and organized without you having to worry about it.



6. PAY-FOR-USE MODEL:

   DynamoDB operates on a pay-as-you-go model. You only pay for the resources you use, making it cost-effective. It's like paying for the exact amount of storage space you need in your filing cabinet without any unnecessary expenses.



In summary, Amazon DynamoDB is a powerful and flexible database service that efficiently stores and retrieves data, scales seamlessly, ensures high availability, and simplifies database management. It's like having a reliable and expandable filing system for your application's data, allowing it to grow and adapt to changing requirements.

Amazon Relational Database Service (RDS) is a managed relational database service offered by Amazon Web Services (AWS). It simplifies the setup, operation, and scaling of relational databases, making it easier for you to focus on your application rather than the complexities of database management. Let's break it down:



1. RELATIONAL DATABASES:

   RDS supports various popular relational database engines such as MySQL, PostgreSQL, Oracle, SQL Server, and MariaDB. These databases are structured and use tables to organize data, making them suitable for applications that require structured and consistent data storage.



2. MANAGED SERVICE:

   RDS takes care of many of the routine database tasks for you. It handles tasks like database setup, patching, backups, and automatic software updates. It's like having a dedicated team that ensures your database is always up to date and secure without you having to manage these tasks manually.



3. SCALABILITY:

   RDS allows you to easily scale your database resources as your application grows. Whether you need more processing power, storage, or both, you can scale up or down with just a few clicks. It's like having the flexibility to expand your storage space or upgrade your computer's performance as your needs change.



4. HIGH AVAILABILITY:

   RDS supports high availability configurations, which means it automatically sets up and manages standby replicas of your database in different locations. If the primary database encounters an issue, RDS seamlessly switches to a standby replica, ensuring continuous access to your data. It's like having a backup office that takes over when your main office has a temporary issue.



5. SECURITY FEATURES:

   RDS includes security features such as encryption at rest and in transit, automated backups, and the ability to define access controls. These features help keep your data secure and comply with regulatory requirements. It's like having a secure vault for your sensitive information with controlled access and regular security checks.



6. MULTI-AZ DEPLOYMENTS:

   For additional reliability, RDS supports Multi-AZ (Availability Zone) deployments. This means your database is replicated in multiple availability zones, providing redundancy and fault tolerance. It's like having copies of your important documents stored in different secure places to ensure they are always accessible.



7. AUTOMATED BACKUPS:

   RDS automatically takes regular backups of your database, making it easy to restore to a specific point in time. It's like having a time machine for your data, allowing you to roll back to a previous state in case of data corruption or mistakes.



In summary, Amazon RDS is a managed service that simplifies the administration of relational databases. It provides scalability, high availability, security features, and automated management, allowing you to focus on building and improving your applications without the hassle of manual database maintenance.

Amazon Neptune is a fully managed graph database service offered by Amazon Web Services (AWS). It's designed to store and query highly connected data, making it ideal for applications that need to navigate and analyze complex relationships between different entities. Here's a breakdown:



1. GRAPH DATABASE:

   Neptune is specifically tailored for handling graph data. In a graph database, information is stored as nodes and edges, representing entities and relationships. This makes it powerful for applications that involve intricate connections and dependencies between data points.



2. MANAGED SERVICE:

   Similar to other AWS services, Neptune is a managed service. This means AWS takes care of the heavy lifting, handling tasks like database setup, maintenance, backups, and updates. It's like having a dedicated team to manage the intricate details of your graph database, allowing you to focus on utilizing the data.



3. SUPPORT FOR GREMLIN AND SPARQL:

   Neptune supports both the Gremlin query language (for property graph databases) and SPARQL (for RDF graph databases). This flexibility allows you to choose the query language that best fits your application's requirements. It's like having two different tools in your toolkit, each specialized for different aspects of graph data.



4. SCALABILITY:

   As your graph data grows, Neptune can scale horizontally to handle increased demand. This scalability ensures that your application remains responsive, even as the complexity of your graph relationships expands. It's like seamlessly adding more lanes to a highway to accommodate increasing traffic.



5. HIGH AVAILABILITY:

   Neptune supports high availability configurations, distributing your data across multiple Availability Zones. In case of an issue in one zone, Neptune automatically redirects requests to a healthy zone, ensuring continuous access to your graph data. It's like having multiple pathways to your destination, allowing you to reach your data even if one route is temporarily blocked.



6. SECURITY FEATURES:

   Neptune incorporates security features such as encryption at rest and in transit, identity and access management controls, and network isolation. These features help safeguard your graph data and ensure compliance with security standards. It's like having a secure vault for your interconnected data, protected by multiple layers of security.



7. SUPPORT FOR RDF AND PROPERTY GRAPHS:

   Neptune is versatile, supporting both Resource Description Framework (RDF) for semantic graph data and Property Graph model for more traditional graph databases. This versatility allows you to choose the graph model that aligns with your specific use case. It's like having different modes on a tool, allowing you to adapt to various graph data scenarios.



In summary, Amazon Neptune is a managed graph database service that excels in handling interconnected data. With support for multiple query languages, scalability, high availability, and robust security features, Neptune simplifies the complexities of managing a graph database, making it easier for you to leverage the power of connected data in your applications.


AWS global infrastructure--

Amazon Route 53 is a scalable and highly available Domain Name System (DNS) web service provided by Amazon Web Services (AWS). DNS is the system that translates human-readable domain names, like www.example.com, into IP addresses that computers use to identify each other on the internet. Route 53 plays a crucial role in managing and routing internet traffic efficiently. Here's a simple introduction:



1. DOMAIN REGISTRATION:

   Route 53 allows you to register and manage domain names. Think of it as the service where you can claim your unique piece of the internet by registering a domain like www.yourbusiness.com.



2. DNS SERVICE:

   DNS is like the internet's address book. Route 53 helps you manage the DNS records that associate your domain with the corresponding IP addresses of your web servers, ensuring that users can find and reach your website.



3. GLOBAL AVAILABILITY:

   Route 53 operates on a global scale, with a network of servers distributed around the world. This global infrastructure ensures low latency and high availability for your domain, regardless of where your users are located.



4. TRAFFIC ROUTING:

   Route 53 enables you to route traffic to different endpoints based on various factors, such as geographic location, health of the endpoint, or the content of the request. This feature allows you to optimize the user experience and enhance the performance of your applications.



5. HEALTH CHECKS:

   Route 53 can monitor the health of your applications and endpoints. If it detects that one of your servers or services is not responding correctly, it can automatically redirect traffic to healthy endpoints, improving the reliability of your applications.



6. SCALABILITY:

   Route 53 seamlessly scales to handle varying levels of internet traffic. Whether you're running a small website or a global application with millions of users, Route 53 can dynamically scale to meet your needs.



7. INTEGRATION WITH AWS SERVICES:

   Route 53 integrates with other AWS services, making it easy to connect your domain to services such as Amazon S3, Elastic Load Balancers, and EC2 instances. This integration simplifies the management of your resources on AWS.



8. SECURITY FEATURES:

   Route 53 includes security features such as DNSSEC (Domain Name System Security Extensions) to help protect against DNS spoofing and other security threats. It's like having a secure gatekeeper for your domain, ensuring that only authorized traffic reaches your applications.



In summary, Amazon Route 53 is a versatile DNS web service that simplifies the management of domain names, optimizes traffic routing, enhances application reliability, and seamlessly integrates with other AWS services. It's a foundational component for establishing a reliable and performant online presence.

Amazon Route 53 as a registrar provides domain registration services, allowing you to secure and manage your internet domain names. Here's an overview:



1. Domain Registration:

   Route 53 allows you to register new domain names, such as www.yourcompany.com. This registration process gives you ownership and control over the specified domain.



2. Domain Transfer:

   If you already have a domain registered with another registrar, Route 53 enables you to transfer it to their service. This consolidation simplifies domain management by having all your domains in one place.



3. Domain Renewal:

   Route 53 handles the renewal of your registered domains. It ensures that your ownership of the domain remains active, preventing it from expiring and becoming available to others.



4. Domain Management Console:

   Route 53 provides a user-friendly console for managing your registered domains. This console allows you to view and update domain details, configure DNS settings, and perform various administrative tasks related to your domains.



5. DNS Management:

   In addition to domain registration services, Route 53 acts as a DNS service. This means you can configure DNS settings for your domains, directing traffic to the appropriate servers and services associated with your domain.



6. Integration with AWS Services:

   Route 53 seamlessly integrates with other AWS services. You can easily connect your registered domains to AWS resources, such as Amazon S3 buckets, Elastic Load Balancers, or EC2 instances. This integration streamlines the process of associating your domains with specific services.



7. Privacy Protection:

   Route 53 provides domain privacy protection, also known as WHOIS privacy. This service helps protect your personal information by replacing your contact details in the public WHOIS database with generic information, reducing the risk of unwanted solicitations or privacy concerns.



8. Automatic Renewals:

   To ensure continuous ownership of your domains, Route 53 supports automatic renewals. This feature simplifies the management process by automatically extending the registration of your domains, preventing unintentional expiration.



In summary, Amazon Route 53 as a registrar offers domain registration and management services, making it easy to secure, renew, and administer your internet domain names. The integration with AWS services further enhances the flexibility and convenience of managing your online presence.

In the realm of Domain Name System (DNS), a Record Set is a collection of DNS records that share the same name and type. DNS is like the phonebook of the internet, translating human-readable domain names (e.g., www.example.com) into IP addresses that computers use to identify each other on the network.



Different types of records in a Record Set provide various information. Here are a few common DNS record types:



1. A Record (Address Record): Maps a domain to an IPv4 address.

2. AAAA Record (IPv6 Address Record): Maps a domain to an IPv6 address.

3. CNAME Record (Canonical Name Record): Alias of one domain to another.

4. MX Record (Mail Exchange Record): Specifies mail servers responsible for receiving email.

5. TXT Record (Text Record): Holds text information, often for verification purposes.

6. NS Record (Name Server Record): Specifies authoritative DNS servers for the domain.



For example, if you own the domain "example.com" and want to set up a website with the address "www.example.com," you would create an A Record in the DNS settings that points "www" to the IP address of your web server.



Routing policies, in the context of networking, refer to rules and configurations that determine how network traffic is directed from one point to another. These policies are crucial for optimizing network performance, ensuring reliability, and implementing security measures. Here are some key aspects:



1. Load Balancing: Distributes network traffic across multiple servers to prevent overload and improve responsiveness. Common algorithms include round-robin, least connections, and IP hash.

2. Failover: Redirects traffic to alternative servers or routes in case of primary server failure. This ensures continuous service availability.

3. Quality of Service (QoS): Prioritizes and manages network traffic to meet specific performance requirements. This is crucial for applications with varying bandwidth needs.

4. Security Policies: Defines rules for traffic filtering and access control to protect the network from unauthorized access, malicious activities, and potential threats.

5. Traffic Engineering: Optimizes network performance by dynamically adjusting routing paths based on real-time conditions, such as congestion or latency.

6. Geographic Routing: Directs traffic based on the geographical location of the user or server, improving efficiency and reducing latency.



For example, a routing policy might be configured to use load balancing to distribute incoming web traffic across multiple servers, ensuring each server shares the load and can handle the requests efficiently.



In summary, Record Sets in DNS manage the mapping of domain names to IP addresses, while Routing Policies in networking determine how data packets are directed through a network to reach their destination, considering factors like load balancing, failover, security, and optimization. These concepts are fundamental in designing and maintaining a robust and efficient network infrastructure

-- Infrastructure mgmt--

 AWS CloudFormation is a service provided by Amazon Web Services (AWS) that enables you to define and provision your AWS infrastructure as code. This means you can use a template to describe all the resources and properties needed for your application stack, and CloudFormation will then take care of creating and managing those resources.



Here are the key concepts to understand about AWS CloudFormation:



### 1. **Templates:**

   - **Definition:** CloudFormation uses templates written in JSON or YAML to describe the AWS resources and properties needed for your application.

   - **Declarative:** Templates are declarative, meaning you specify what resources you want and their configuration, rather than the step-by-step process of how to create them.



### 2. **Stacks:**

   - **Definition:** A stack is a collection of AWS resources that you can manage as a single unit. It's created and managed as part of the CloudFormation process.

   - **Lifecycle:** CloudFormation manages the entire lifecycle of a stack, from creation to updates and deletion.



### 3. **Resources:**

   - **Definition:** Resources are the AWS components that you declare in your template, such as EC2 instances, S3 buckets, databases, and more.

   - **Dependency Management:** CloudFormation understands the dependencies between resources and deploys them in the correct order.



### 4. **Parameters:**

   - **Definition:** Parameters allow you to input custom values into your template when you create or update a stack. This makes your templates more flexible and reusable.

   - **Dynamic Configurations:** Parameters enable you to customize your stack based on different needs, environments, or configurations.



### 5. **Outputs:**

   - **Definition:** Outputs allow you to retrieve information about your stack, such as the public IP address of an EC2 instance or the URL of a deployed application.

   - **Communication Between Stacks:** Outputs facilitate communication between different stacks or with external systems.



### 6. **Change Sets:**

   - **Definition:** Before making changes to a stack, CloudFormation provides a preview of how the changes will affect your resources. This is known as a Change Set.

   - **Safety Mechanism:** Change Sets help you understand the impact of changes and ensure that you don't inadvertently make destructive updates.



### 7. **Rollbacks:**

   - **Definition:** If there is an issue during a stack update, CloudFormation can automatically roll back to the previous known good state.

   - **Safety Net:** Rollbacks provide a safety net to prevent your infrastructure from being in an inconsistent or broken state.



### 8. **Integration with AWS Services:**

   - **Definition:** CloudFormation integrates with various AWS services, allowing you to model and provision a wide range of resources.

   - **Consistent Infrastructure:** It ensures consistent and reproducible infrastructure deployments, reducing the risk of configuration drift.



### 9. **Version Control Integration:**

   - **Definition:** CloudFormation templates can be stored and versioned in source control systems, such as Git.

   - **Collaboration:** Version control integration enables collaboration among team members and provides a history of changes to the infrastructure over time.



In summary, AWS CloudFormation simplifies and automates the process of managing and provisioning AWS infrastructure. 
It allows you to treat your infrastructure as code, making it easier to maintain, version, and replicate across different environments. 
This is especially valuable in the context of scalable and dynamic cloud-based applications.


AWS Elastic Beanstalk is a Platform as a Service (PaaS) offering by Amazon Web Services (AWS) that makes it easy to deploy and run applications in various programming languages on the AWS infrastructure. It abstracts away much of the underlying infrastructure management, allowing developers to focus more on writing code and less on managing servers.



Here are the key concepts of AWS Elastic Beanstalk:



### 1. **Platform as a Service (PaaS):**

   - **Definition:** Elastic Beanstalk is a PaaS offering, which means it provides a platform for developers to build, deploy, and scale applications without dealing with the complexities of underlying infrastructure.



### 2. **Managed Environments:**

   - **Definition:** Elastic Beanstalk provides managed environments for different types of applications, such as web applications, APIs, and worker applications.

   - **Configuration-Free Environment Management:** Developers can deploy their applications without worrying about configuring the underlying infrastructure.



### 3. **Multi-Language Support:**

   - **Definition:** Elastic Beanstalk supports multiple programming languages, including Java, .NET, Node.js, Python, Ruby, Go, and more.

   - **Choice of Technology:** Developers can choose the language and runtime that best suits their application.



### 4. **Application Versions:**

   - **Definition:** Applications in Elastic Beanstalk are organized into versions. Each version represents a deployable iteration of the application.

   - **Easy Rollbacks:** You can easily roll back to a previous version if an issue arises after a deployment.



### 5. **Environment Configuration:**

   - **Definition:** Elastic Beanstalk environments can be configured with various settings, such as instance types, scaling options, security settings, and environment variables.

   - **Scalability:** It provides easy scaling options, allowing you to scale up or down based on demand.



### 6. **Managed Deployment:**

   - **Definition:** Elastic Beanstalk simplifies the deployment process. You can deploy applications using the AWS Management Console, the AWS Command Line Interface (CLI), or through an integrated development environment (IDE).

   - **Continuous Deployment:** Integration with AWS CodePipeline and other CI/CD tools allows for continuous deployment workflows.



### 7. **Auto Scaling:**

   - **Definition:** Elastic Beanstalk includes built-in Auto Scaling, which automatically adjusts the number of running instances based on traffic and demand.

   - **Optimizing Resource Usage:** Auto Scaling helps optimize resource usage and ensures that the application can handle varying levels of load.



### 8. **Logging and Monitoring:**

   - **Definition:** Elastic Beanstalk provides integrated logging and monitoring through AWS CloudWatch. This includes metrics on request latency, error rates, and more.

   - **Performance Insights:** Monitoring helps developers gain insights into the performance of their applications.



### 9. **Integration with Other AWS Services:**

   - **Definition:** Elastic Beanstalk integrates seamlessly with other AWS services, such as Amazon RDS (Relational Database Service), Amazon S3 (Simple Storage Service), and AWS Identity and Access Management (IAM).

   - **Leveraging Ecosystem:** Developers can leverage the broader AWS ecosystem to add functionalities like databases, storage, and security features.



### 10. **Ease of Management:**

   - **Definition:** Elastic Beanstalk abstracts away much of the complexity of managing infrastructure, making it easier for developers to focus on coding and application development.

   - **Reduced Administrative Overhead:** Developers don't need to worry about tasks like patching operating systems or managing server instances.



In summary, AWS Elastic Beanstalk is a powerful service that simplifies the process of deploying and managing applications on AWS. 
It's particularly beneficial for developers who want to quickly deploy applications without getting into the nitty-gritty details of 
infrastructure management. With support for multiple programming languages and seamless integration with other AWS services, 
Elastic Beanstalk provides a straightforward path for deploying scalable and reliable applications in the cloud.

The AWS services you've mentioned—CodeDeploy, CodeCommit, CodeBuild, CodePipeline, and CodeArtifact—are part of the AWS DevOps toolchain. Let's break down each one:



 1. AWS CodeDeploy:

   - **Definition:** AWS CodeDeploy is a service that automates the deployment of applications to a variety of compute services, including EC2 instances, Lambda functions, and instances running on-premises.

   - Key Features:

      - **Flexible Deployment:** CodeDeploy supports various deployment strategies, such as rolling deployments and blue-green deployments, allowing you to update applications with minimal downtime.

      - **Integration:** It integrates with other AWS services and supports deployments from source code repositories, including AWS CodeCommit and GitHub.



2. **AWS CodeCommit:**

   - **Definition:** AWS CodeCommit is a fully managed source control service that makes it easy for teams to host secure and scalable Git repositories.

   - **Key Features:**

      - **Secure and Scalable:** CodeCommit provides a secure and scalable Git repository hosting solution with support for encryption in transit and at rest.

      - **Integration:** It integrates seamlessly with other AWS services, including CodeBuild, CodePipeline, and CodeDeploy, allowing for a continuous integration and continuous delivery (CI/CD) workflow.



3. **AWS CodeBuild:**

   - **Definition:** AWS CodeBuild is a fully managed build service that compiles source code, runs tests, and produces software packages that are ready to deploy.

   - **Key Features:**

      - **Build Automation:** CodeBuild automates the build and test phases of your release process based on configurations defined in build specifications.

      - **Scalability:** It scales automatically, enabling you to build and test code quickly, with the ability to run builds concurrently.



4. **AWS CodePipeline:**

   - **Definition:** AWS CodePipeline is a continuous integration and continuous delivery (CI/CD) service that automates the build, test, and deployment phases of your release process.

   - **Key Features:**

      - **Workflow Automation:** CodePipeline allows you to define and visualize your release process, including integration with various source control and build services.

      - **Integration with Other Services:** It integrates seamlessly with other DevOps tools, such as CodeBuild, CodeDeploy, and third-party services, providing end-to-end automation of your release process.



 5. **AWS CodeArtifact:**

   - **Definition:** AWS CodeArtifact is a fully managed artifact repository service that allows you to store, publish, and share software packages.

   - **Key Features:**

      - **Centralized Artifact Management:** CodeArtifact provides a centralized repository for managing software artifacts, making it easier to manage dependencies and share packages across development teams.

      - **Integration:** It integrates with popular build tools and package managers, such as Maven, npm, and NuGet, allowing you to easily fetch and publish artifacts.



Summary:

- **Workflow Integration:** CodeCommit is used for source code management, CodeBuild for building and testing code, CodeDeploy for deploying applications, CodePipeline for orchestrating the entire CI/CD workflow, and CodeArtifact for managing artifacts.

  

- **End-to-End Automation:** When combined, these services create an end-to-end automation pipeline, from source code management through building and testing to deployment, ensuring a streamlined and efficient software development lifecycle.



- **Scalability and Flexibility:** The services are designed to scale with your application needs and provide flexibility in choosing your preferred development tools and workflows.



- **Interconnected Ecosystem:** The seamless integration between these services allows for a cohesive and automated development and deployment process, 
promoting best practices in DevOps and CI/CD methodologies


AWS Systems Manager (SSM) is a service provided by Amazon Web Services. It helps you manage your AWS resources and applications in a centralized and efficient manner. Instead of navigating through different services individually, SSM provides a unified interface for operations and automation tasks. Here are some key aspects:



1. Resource Management:

   - SSM allows you to view and manage your AWS resources from a central location, making it easier to oversee and operate your infrastructure.



2. Automation:

   - It offers automation capabilities for repetitive tasks and workflows. You can create and run automation documents, which are sets of predefined steps to perform specific actions.



3. Run Command:

   - SSM's Run Command feature lets you remotely execute commands on your instances, eliminating the need for direct access. This is useful for tasks like software installations or configuration changes.



4. State Manager:

   - State Manager helps you define and maintain a consistent state for your instances. You can specify configurations and apply them to ensure your instances comply with desired settings.



5. Inventory:

   - SSM provides an inventory of your resources, giving you insight into the software installed on your instances and other configuration details.



6. Parameter Store:

   - Parameter Store within SSM allows you to securely store and manage configuration data such as database connection strings, API keys, or other sensitive information.



7. Session Manager:

   - Session Manager enables secure and audited remote access to your instances without the need for open inbound ports. This enhances security by reducing the attack surface.



8. Maintenance Windows:

   - You can use Maintenance Windows to define a schedule for when you want to perform operational tasks, such as updates and patches, making it easier to manage maintenance activities.



9. Insights:

   - SSM provides insights into the operational data of your instances, helping you identify and resolve issues more efficiently.



10. Integration with Other AWS Services:

    - SSM integrates with other AWS services, allowing you to leverage its capabilities in conjunction with services like AWS Identity and Access Management (IAM), AWS CloudWatch, and AWS Lambda.



In summary, AWS Systems Manager is a comprehensive tool that simplifies the management and automation of your AWS infrastructure. It centralizes tasks, offers automation options, and provides features for configuration management, inventory tracking, and secure remote access, contributing to efficient and secure operations in the AWS cloud environment.

--Applicaton Integrationn Services--


Amazon API Gateway is a service provided by Amazon Web Services (AWS) that facilitates the creation, management, and deployment of 
Application Programming Interfaces (APIs). APIs serve as interfaces that allow different software applications to communicate and share data. 
Here are the key aspects of Amazon API Gateway:



1. API Creation:

   - Amazon API Gateway enables you to create APIs quickly and easily. You can define the structure of your API, set up endpoints, and specify how data should be processed.



2. RESTful and WebSocket APIs:

   - It supports both RESTful APIs, which are commonly used for web services, and WebSocket APIs, which enable real-time communication between clients and servers.



3. Integration with AWS Services:

   - Amazon API Gateway seamlessly integrates with various AWS services, allowing you to connect your API to resources such as AWS Lambda functions, Amazon S3 buckets, or other HTTP endpoints.



4. Security Features:

   - API Gateway provides a range of security features, including authentication and authorization mechanisms. You can control access to your APIs using AWS Identity and Access Management (IAM) and implement OAuth or API keys for additional security.



5. Throttling and Rate Limiting:

   - To manage traffic and prevent abuse, API Gateway allows you to set up throttling and rate limiting. This ensures fair usage and protects your backend resources from being overwhelmed.



6. Monitoring and Logging:

   - API Gateway offers tools for monitoring the performance of your APIs. You can access logs, metrics, and tracing information to gain insights into how your APIs are being used.



7. Custom Domain Names:

   - You can configure custom domain names for your APIs, making it easier to establish a branded and easily recognizable API endpoint.



8. Deployment and Versioning:

   - API Gateway supports easy deployment of API changes. You can create different versions of your API, allowing you to roll out updates without disrupting existing applications.



9. CORS Support:

   - Cross-Origin Resource Sharing (CORS) is supported, enabling you to define which domains are permitted to access your APIs.



10. Cost Management:

    - API Gateway offers a pay-as-you-go pricing model, allowing you to manage costs effectively based on the usage of your APIs.



In summary, Amazon API Gateway simplifies the process of creating, deploying, and managing APIs. It provides a scalable and secure platform for building APIs that can integrate with various backend services. Whether you're creating a RESTful API for a web application or a real-time WebSocket API for a chat application, API Gateway streamlines the development and management of these crucial interfaces.


Amazon EventBridge is a service provided by Amazon Web Services (AWS) that simplifies the management of events and 
data streams within your applications. It enables the creation, routing, and processing of events from various sources, 
allowing for seamless communication and coordination between different components. Here are the key aspects of Amazon EventBridge:



1. Event-Driven Architecture:

   - Amazon EventBridge facilitates an event-driven architecture, where different components of your applications communicate through events. This promotes decoupling and flexibility in the development process.



2. Event Sources:

   - Event sources can include AWS services, integrated software as a service (SaaS) applications, or custom sources. These sources generate events that can trigger actions within your application.



3. Event Buses:

   - EventBridge uses event buses to route and deliver events. An event bus is a communication channel that allows different components to send and receive events.



4. Rules:

   - Rules in EventBridge define the conditions under which events are routed and processed. You can create rules to filter and route events based on attributes, patterns, or specific criteria.



5. Schema Registry:

   - EventBridge includes a schema registry that allows you to define the structure of events in a central location. This helps ensure consistency and compatibility across different components.



6. Integration with AWS Services:

   - EventBridge seamlessly integrates with various AWS services, such as AWS Lambda, Amazon SNS, and AWS Step Functions. This allows you to connect events to specific actions or workflows.



7. Custom Event Buses:

   - You can create custom event buses to segment and organize events based on your application's needs. This helps in managing and scaling complex event-driven architectures.



8. Event Replay:

   - EventBridge provides event replay capabilities, allowing you to replay past events for debugging, testing, or reprocessing purposes.



9. Cross-Account Event Bus:

   - EventBridge supports the creation of cross-account event buses, enabling communication and event sharing between different AWS accounts.



10. Monitoring and Insights:

    - EventBridge offers monitoring and logging features through AWS CloudWatch. You can track and analyze events, monitor the performance of your event-driven applications, and gain insights into the flow of events.



In summary, Amazon EventBridge is a powerful service that simplifies the management of events in your applications. It promotes a flexible and scalable event-driven architecture, allowing for efficient communication and coordination between different components. Whether you're working with AWS services or custom applications, EventBridge provides a unified and streamlined approach to handling events and data streams.

Amazon Simple Notification Service (SNS) is a fully managed messaging service provided by Amazon Web Services (AWS). 
It facilitates the communication and delivery of messages or notifications to a distributed set of recipients or endpoints. 
Here are the key aspects of Amazon SNS:



1. Publish-Subscribe Model:

   - SNS operates on a publish-subscribe model, allowing message publishers to send messages to multiple subscribers. Subscribers can receive messages in real-time.



2. Topics:

   - Communication is organized into topics, which act as communication channels. Publishers send messages to specific topics, and subscribers receive messages from topics they've subscribed to.



3. Subscribers:

   - Subscribers are the endpoints that receive messages. These endpoints can include applications, AWS Lambda functions, SQS queues, HTTP endpoints, or even email addresses.



4. Protocols:

   - SNS supports various protocols for message delivery, including HTTP, HTTPS, email, SMS, and application protocols such as Amazon SQS and AWS Lambda.



5. Message Filtering:

   - SNS allows subscribers to filter messages based on specific criteria. This helps in tailoring the content of messages that subscribers receive.



6. Delivery Retries:

   - In case of delivery failures, SNS automatically retries message delivery multiple times. This ensures the robustness and reliability of message delivery.



7. Message Attributes:

   - Publishers can attach attributes to messages for additional metadata. This allows for the inclusion of information such as message type or priority.



8. Message Encryption:

   - SNS supports message encryption to ensure the security of sensitive information during message transmission.



9. Mobile Push Notifications:

   - SNS provides support for mobile push notifications, allowing you to send messages to mobile devices, including iOS, Android, and other platforms.



10. Integration with Other AWS Services:

    - SNS seamlessly integrates with other AWS services, allowing you to use it as a communication bridge between different components in your AWS architecture.



In summary, Amazon Simple Notification Service (SNS) is a versatile and scalable messaging service that simplifies the distribution of messages or notifications across various endpoints. Its flexibility, support for multiple protocols, and integration with other AWS services make it a valuable tool for building robust and efficient communication systems in the cloud.

Amazon Simple Queue Service (SQS) is a fully managed message queuing service provided by Amazon Web Services (AWS). It enables decoupling of the components in a distributed system by allowing them to communicate asynchronously through the use of messages. Here are the key aspects of Amazon SQS:



1. Messaging Queue:

   - SQS is designed as a messaging queue, allowing one component (producer) to send messages to a queue, and another component (consumer) to retrieve and process those messages.



2. Decoupling Components:

   - SQS facilitates the decoupling of different parts of a system, meaning that components can work independently without having to directly interact with each other.



3. Queue Types:

   - SQS offers two types of queues: Standard Queues and FIFO (First-In-First-Out) Queues. Standard Queues provide at-least-once message delivery, while FIFO Queues guarantee exactly-once processing.



4. Message Retention:

   - SQS retains messages in a queue for a configurable period, even if they have been retrieved by a consumer. This ensures that messages are not lost and can be reprocessed if needed.



5. Scalability:

   - SQS is highly scalable and can handle a large number of messages per second. This scalability makes it suitable for applications with varying workloads.



6. Fault Tolerance:

   - SQS is designed to be highly fault-tolerant. It replicates messages across multiple Availability Zones to ensure durability and availability.



7. Message Visibility Timeout:

   - When a consumer retrieves a message from a queue, SQS makes the message invisible to other consumers for a specified period (visibility timeout). This prevents multiple consumers from processing the same message simultaneously.



8. Dead-Letter Queues:

   - SQS allows you to configure dead-letter queues where messages that cannot be processed successfully are sent. This helps in identifying and handling problematic messages.



9. Long Polling:

   - SQS supports long polling, which allows consumers to wait for messages to arrive in an empty queue without incurring extra costs. This reduces the number of empty responses and provides more efficient message retrieval.



10. Integration with AWS Services:

    - SQS integrates seamlessly with other AWS services, including AWS Lambda, Amazon S3, and Amazon EC2. This allows you to build scalable and resilient applications by combining different AWS services.



In summary, Amazon Simple Queue Service (SQS) is a foundational service for building scalable and loosely coupled distributed systems. It provides a reliable and scalable mechanism for asynchronous communication between different components, allowing for increased flexibility, fault tolerance, and scalability in cloud-based applications.


Amazon MQ is a fully managed message broker service provided by Amazon Web Services (AWS). It enables the communication and exchange of messages between different software components in a distributed system. Here are the key aspects of Amazon MQ:



1. Message Broker:

   - Amazon MQ acts as a message broker, facilitating the exchange of messages between various components of a distributed application. It helps decouple different parts of the system, promoting flexibility and scalability.



2. Compatibility with Message Protocols:

   - Amazon MQ supports industry-standard messaging protocols such as MQTT, AMQP, and STOMP. This allows applications developed with different programming languages and technologies to communicate seamlessly.



3. Managed Service:

   - Amazon MQ is a fully managed service, meaning that AWS takes care of the underlying infrastructure, including maintenance, updates, and scaling. This allows developers to focus on building applications rather than managing infrastructure.



4. Queue and Topic Models:

   - Amazon MQ supports both queue and topic-based messaging models. Queues are used for point-to-point communication, while topics facilitate publish-subscribe patterns for broadcasting messages to multiple subscribers.



5. Integration with Existing Applications:

   - Amazon MQ is designed to integrate with existing applications and services, making it easy to migrate or connect your applications to the messaging service without significant changes.



6. Secure Communication:

   - Security features such as encryption in transit and at rest, access control, and integration with AWS Identity and Access Management (IAM) ensure the confidentiality and integrity of messages.



7. Horizontal Scaling:

   - Amazon MQ allows for horizontal scaling, meaning you can adjust the capacity of your message broker to handle varying workloads. This ensures that your messaging infrastructure can scale with the needs of your application.



8. High Availability:

   - Amazon MQ provides high availability by deploying across multiple Availability Zones. This redundancy ensures that the messaging service remains available even in the event of a failure in one of the data centers.



9. Monitoring and Logging:

   - Amazon MQ offers monitoring capabilities through Amazon CloudWatch, providing insights into the performance and health of the message broker. Logs help in diagnosing issues and tracking the flow of messages.



10. Flexible Deployment Models:

    - Amazon MQ supports different deployment models, allowing you to choose between single-instance brokers for basic use cases or active-active clusters for high availability and increased throughput.



In summary, Amazon MQ is a managed message broker service that simplifies the implementation of messaging patterns in distributed systems. Its compatibility with standard protocols, managed nature, security features, and integration capabilities make it a valuable tool for building scalable, reliable, and loosely coupled applications in the cloud.

AWS Step Functions is a fully managed service provided by Amazon Web Services (AWS) that simplifies the orchestration of microservices-based workflows and applications. It allows you to design, visualize, and execute complex workflows composed of multiple services, enabling you to build scalable and resilient applications. Here are the key aspects of AWS Step Functions:



1. Workflow Orchestration:

   - AWS Step Functions enables you to orchestrate and coordinate the execution of different services and functions within your application. It provides a way to define and visualize the flow of tasks in a workflow.



2. State Machines:

   - Workflows in AWS Step Functions are defined as state machines. Each state represents a step in the workflow, and transitions between states define the flow of execution.



3. Supported Integrations:

   - Step Functions integrates seamlessly with a variety of AWS services, including AWS Lambda, Amazon S3, Amazon DynamoDB, AWS Glue, and more. This allows you to combine serverless functions, services, and custom code in your workflows.



4. Visual Workflow Editor:

   - The visual workflow editor in the AWS Management Console allows you to design and modify workflows graphically. This visual representation makes it easier to understand and manage the flow of your applications.



5. Retry and Error Handling:

   - AWS Step Functions includes built-in capabilities for handling errors and retries. You can define error-catching states and specify retry policies to manage transient failures in your workflows.



6. Parallel Execution:

   - Workflows often involve executing tasks in parallel. Step Functions allows you to define parallel branches, where multiple tasks can be executed concurrently, improving the efficiency of your workflows.



7. Wait States:

   - You can incorporate wait states in your workflows to introduce delays or wait for external events before proceeding to the next state. This is useful for coordinating timing-sensitive tasks.



8. Dynamic Workflows:

   - Step Functions support dynamic workflows by allowing you to use parameters and input/output data from states. This flexibility enables you to create reusable and adaptable workflows.



9. Integration with Step Functions Data:

   - Step Functions maintains execution history and data, making it easier to track the progress of workflows and analyze historical data for insights into application behavior.



10. Logging and Monitoring:

    - AWS Step Functions integrates with AWS CloudWatch for logging and monitoring. You can monitor the execution of your workflows, set up alarms, and gain insights into performance.



In summary, AWS Step Functions is a powerful service for orchestrating and managing the execution of workflows in AWS. Whether you are building microservices-based applications or coordinating serverless functions, Step Functions provides a scalable and resilient solution for workflow orchestration, making it easier to develop and maintain complex applications in the cloud.


--Cloud Monitoring---

Amazon CloudWatch is a monitoring and observability service provided by Amazon Web Services (AWS). It is designed to help you collect, analyze, and visualize data from various AWS resources and applications. Here are the key aspects of Amazon CloudWatch:



1. Metrics:

   - CloudWatch collects and stores metrics, which are numerical data points that represent the performance and behavior of your AWS resources. These metrics could include CPU utilization, network traffic, or the number of requests to a service.



2. Dashboard:

   - CloudWatch Dashboards allow you to create customized visualizations of your metrics. You can arrange graphs, charts, and other widgets to provide a quick overview of the health and performance of your AWS resources.



3. Alarms:

   - Alarms in CloudWatch allow you to set thresholds on your metrics. When a metric breaches a threshold, an alarm is triggered, and you can configure actions such as sending notifications or automatically adjusting AWS resources.



4. Logs:

   - CloudWatch Logs enable you to collect, store, and analyze log files from your applications and AWS resources. This is useful for debugging, troubleshooting, and auditing purposes.



5. Events:

   - CloudWatch Events provide a way to respond to changes in your AWS environment. You can create rules that match events and trigger actions, such as invoking an AWS Lambda function or sending a notification.



6. Retrieval and Storage of Data:

   - CloudWatch retains metric data for a configurable period, allowing you to analyze historical trends and performance over time. The retention period varies based on the granularity of the data.



7. Custom Metrics:

   - In addition to the metrics automatically collected by AWS, you can publish custom metrics to CloudWatch. This allows you to monitor specific aspects of your applications that are relevant to your business requirements.



8. Integration with AWS Services:

   - CloudWatch seamlessly integrates with various AWS services, allowing you to monitor and collect data from resources such as Amazon EC2 instances, Amazon RDS databases, and AWS Lambda functions.



9. Cross-Account and Cross-Region Monitoring:

   - CloudWatch enables you to set up cross-account and cross-region dashboards, alarms, and metrics. This is beneficial for monitoring resources that span multiple AWS accounts and regions.



10. Application Insights:

    - CloudWatch Application Insights helps you monitor and troubleshoot your applications. It automatically detects anomalies, identifies potential issues, and provides insights into the performance of your applications.



In summary, Amazon CloudWatch is a comprehensive monitoring service that plays a crucial role in managing and maintaining the health, performance, and reliability of your AWS resources and applications. Whether you're tracking basic metrics or implementing sophisticated monitoring solutions, CloudWatch provides the tools to ensure your AWS environment operates efficiently and reliably.


Amazon CloudTrail is a service provided by Amazon Web Services (AWS) that enables governance, compliance, operational auditing, and risk auditing of your AWS account. It records API calls made on your AWS account, capturing important information about these calls. Here are the key aspects of Amazon CloudTrail:



1. Audit Trail:

   - CloudTrail provides an audit trail of API calls made on your AWS account. This includes calls made through the AWS Management Console, AWS Command Line Interface (CLI), SDKs, and other AWS services.



2. Event History:

   - It maintains a detailed event history that includes information such as the identity of the caller, the time of the API call, the source IP address, the request parameters, and the response elements returned by AWS.



3. Compliance and Governance:

   - CloudTrail helps you meet compliance requirements and governance objectives by providing a record of actions taken in your AWS environment. This is particularly important for regulated industries or organizations with strict compliance standards.



4. Security Analysis:

   - The recorded data in CloudTrail logs can be used for security analysis, helping you identify unusual or potentially malicious activity in your AWS account. It provides visibility into who did what, and when, across your AWS infrastructure.



5. Log File Integrity:

   - CloudTrail log files are cryptographically signed to ensure their integrity. This helps in verifying that the log files haven't been tampered with or altered, providing a secure and reliable record of API activity.



6. Integration with CloudWatch:

   - CloudTrail can be configured to deliver log files to Amazon CloudWatch Logs. This allows you to perform real-time analysis of log data, set up alerts, and create dashboards for monitoring and troubleshooting.



7. Multi-Region Support:

   - CloudTrail supports multi-region trails, enabling you to capture API activity in multiple AWS regions. This is useful for organizations with a global presence or distributed AWS resources.



8. S3 Bucket Integration:

   - CloudTrail can store log files in an Amazon S3 bucket. This makes it easy to archive, search, and analyze logs using tools of your choice, and it provides a scalable and durable storage solution for your CloudTrail data.



9. CloudTrail Insights:

   - CloudTrail Insights is a feature that helps you identify and respond to unusual operational activity in your AWS account. It uses machine learning algorithms to analyze CloudTrail events and detect anomalies.



10. Event Filtering:

    - You can configure CloudTrail to log only the events you are interested in, reducing the volume of data captured and ensuring that you focus on the most relevant information for your specific use cases.



In summary, Amazon CloudTrail is a foundational tool for maintaining visibility and control over your AWS environment. It provides a comprehensive record of API calls, aiding in compliance, governance, security analysis, and operational troubleshooting.


Cloud Guru (ACG), which is an online learning platform that specializes in cloud computing and AWS (Amazon Web Services) training. If there have been changes or new developments after my last update, I recommend checking the latest information.



If you are indeed referring to A Cloud Guru, here's an explanation:



A Cloud Guru (ACG):

A Cloud Guru is an online education platform that provides courses and training programs focused on cloud computing technologies, with a particular emphasis on AWS. The platform offers a variety of courses, labs, and hands-on projects to help individuals learn and enhance their skills in cloud technologies. Here are key aspects of A Cloud Guru:



Cloud Computing Training:



A Cloud Guru offers courses covering a wide range of cloud computing topics, with a focus on AWS services. This includes fundamental concepts, certification exam preparation, and specialized topics related to cloud architecture, development, and operations.

AWS Certification Preparation:



ACG is known for its comprehensive training programs designed to help individuals prepare for AWS certification exams. The courses are structured to cover the exam objectives and provide practical insights to ensure success in certification endeavors.

Hands-On Labs and Projects:



The platform provides hands-on labs and projects, allowing learners to gain practical experience by working with real cloud environments. This hands-on approach helps reinforce theoretical knowledge with practical skills.

Instructor-Led Training:



A Cloud Guru courses often feature instructional videos led by experienced instructors in the field. These instructors guide learners through the content, providing explanations, demonstrations, and real-world scenarios.

Community and Forums:



ACG fosters a community of learners through forums and discussion groups. This allows individuals to connect, ask questions, and share insights, creating a collaborative learning environment.

Continuous Content Updates:



Given the dynamic nature of cloud technologies, A Cloud Guru regularly updates its content to reflect the latest developments in AWS and cloud computing. This ensures that learners have access to current and relevant information.

Flexible Learning Paths:



ACG offers learning paths catering to different skill levels, from beginners to advanced users. Learners can choose paths based on their goals, whether it's gaining a foundational understanding of cloud concepts or becoming an expert in a specific area.

Mobile Accessibility:



The platform is often accessible through web browsers and mobile apps, allowing learners to access content from various devices and locations.

In summary, A Cloud Guru is an online learning platform designed to help individuals acquire and enhance their skills in cloud computing, particularly with a focus on AWS. It offers a combination of instructional videos, hands-on labs, community interaction, and certification preparation to support a comprehensive learning experience.



--VPC and Networking--

A Virtual Private Cloud (VPC) is a fundamental component of cloud computing infrastructure, particularly within platforms like Amazon Web Services (AWS). It enables users to create isolated and logically defined sections within the cloud where they can deploy and run their resources. Here are the key aspects of Virtual Private Cloud explained:



1. Isolation and Segmentation:

   - A VPC provides a logically isolated section of the cloud where you can launch AWS resources. It acts as a private network in the cloud, allowing you to create a segmented environment for your applications and services.



2. Custom Networking:

   - With a VPC, you have control over your network configuration. You can define your own IP address range, create subnets, configure route tables, and set up network gateways. This flexibility allows you to design a network that meets the specific requirements of your applications.



3. Subnetting:

   - A VPC can be divided into multiple subnets. Subnets are segments of the VPC's IP address range where you can place resources. This allows you to organize and manage your resources based on different security and operational requirements.



4. Internet Connectivity:

   - By default, instances in a VPC are not accessible from the internet. However, you can configure internet access by attaching an Internet Gateway to your VPC. This allows resources within the VPC to communicate with the internet and vice versa.



5. Private Networking:

   - In addition to internet connectivity, you can create private subnets within a VPC that do not have direct access to the internet. This is useful for hosting sensitive workloads that should not be exposed to the public internet.



6. Security Groups and Network Access Control Lists (NACLs):

   - VPCs use security groups and network access control lists (NACLs) to control inbound and outbound traffic at the instance and subnet levels. Security groups are stateful, and NACLs are stateless, providing different layers of security control.



7. Virtual Private Network (VPN) and Direct Connect:

   - VPCs support secure connectivity to on-premises data centers through VPN connections or AWS Direct Connect. This allows you to extend your on-premises network to the cloud securely.



8. Elastic Load Balancers (ELB) and Auto Scaling:

   - VPC integrates with Elastic Load Balancers (ELB) and Auto Scaling, enabling the automatic distribution of incoming application traffic across multiple instances and the ability to scale resources based on demand.



9. Peering and Transit Gateways:

   - VPC peering allows you to connect VPCs together and route traffic between them privately. Transit Gateway is a centralized hub for connecting multiple VPCs and on-premises networks, simplifying network architecture.



10. Resource Deployment:

    - Within a VPC, you can deploy various AWS resources such as Amazon EC2 instances, RDS databases, S3 storage, and more. The VPC acts as the networking foundation that ties these resources together.



In summary, a Virtual Private Cloud (VPC) is a foundational element in cloud computing that provides users with the ability to create a private, isolated, and customizable network in the cloud. It offers control over networking, security, and connectivity, allowing for the deployment of diverse applications and services in a secure and scalable manner.

Security Groups:

A Security Group acts as a virtual firewall for your instances to control inbound and outbound traffic. It is associated with instances, and you can specify rules that control the traffic to and from these instances. Here are key points about Security Groups:



1. Stateful Filtering:

   - Security Groups are stateful, meaning if you allow inbound traffic from a specific IP, the corresponding outbound traffic is automatically allowed. This simplifies rule configuration.



2. Inbound and Outbound Rules:

   - You can define inbound rules to control incoming traffic and outbound rules to control outgoing traffic. Rules are based on protocols, ports, and IP ranges.



3. Allow Rules Only:

   - Security Groups only allow traffic; there are no explicit "deny" rules. If a rule doesn't explicitly permit the traffic, it is implicitly denied.



4. Instance Level:

   - Security Groups are associated with instances, and each instance can have multiple security groups. When an instance is launched, you can specify the security groups it belongs to.



5. Stateful Connection Tracking:

   - The stateful nature of Security Groups means that once a connection is established, the response traffic is automatically allowed, facilitating communication between instances.



6. Implicit Outbound Rules:

   - Outbound traffic is allowed by default. You don't need to specify outbound rules unless you want to restrict certain types of outbound traffic.



7. Changes Take Effect Immediately:

   - Changes to Security Group rules take effect immediately, reducing the risk of misconfigurations.



Network Access Control Lists (NACLs):

Network Access Control Lists are an additional layer of security for your VPC. While Security Groups operate at the instance level, NACLs operate at the subnet level. Here are key points about NACLs:



1. Stateless Filtering:

   - NACLs are stateless, meaning that if you allow inbound traffic, you must also explicitly allow the corresponding outbound traffic. Each rule applies independently.



2. Inbound and Outbound Rules:

   - Similar to Security Groups, NACLs have inbound and outbound rules. Rules are based on protocols, ports, and IP ranges.



3. Explicit Allow and Deny Rules:

   - NACLs support both explicit "allow" and "deny" rules, giving you more control over traffic. Unlike Security Groups, which allow by default, NACLs deny all traffic by default.



4. Ordered Rule Set:

   - Rules in NACLs are evaluated based on their order. The rules are processed in numerical order, starting with the lowest rule number. This allows for precise control over the flow of traffic.



5. Subnet Level:

   - NACLs are associated with subnets, and each subnet in a VPC must be associated with a NACL. A subnet can be associated with only one NACL at a time.



6. Changes Take Effect Immediately:

   - Similar to Security Groups, changes to NACL rules take effect immediately.



7. Logging:

   - NACLs offer the capability to log information about the traffic that matches a rule. This is useful for monitoring and troubleshooting.



In summary, Security Groups and Network Access Control Lists are both essential components of AWS networking and security. Security Groups operate at the instance level and are stateful, while NACLs operate at the subnet level and are stateless. Both are used to control inbound and outbound traffic but offer different features and capabilities. Understanding how to use these effectively is crucial for designing a secure and well-performing AWS environment.


--Security--

Encryption with KMS--


Encryption is the process of converting data into a secure format that can only be read by authorized users or systems. AWS Key Management Service (KMS) is a managed service that makes it easy for you to create and control the encryption keys used to encrypt your data. Here's an explanation of encryption with KMS:



1. Key Generation:

   - KMS allows you to generate cryptographic keys, referred to as Customer Master Keys (CMKs), that are used to encrypt and decrypt your data. CMKs can be either symmetric or asymmetric, depending on your use case.



2. Key Storage and Management:

   - KMS securely stores and manages the keys, relieving you of the responsibility of key management. AWS handles the complexities of key rotation, versioning, and access control.



3. Data Encryption:

   - Once you have a CMK, you can use it to encrypt your sensitive data. For symmetric keys, the same key is used for both encryption and decryption. For asymmetric keys, there are separate public and private keys; data encrypted with the public key can only be decrypted with the corresponding private key.



4. Integration with AWS Services:

   - KMS integrates seamlessly with various AWS services, such as Amazon S3, EBS, RDS, and more. You can enable encryption for these services, and KMS automatically handles key management behind the scenes.



5. Envelope Encryption:

   - KMS employs a technique called envelope encryption. When you request data to be encrypted, KMS uses a data key, which is a one-time-use symmetric key, to encrypt the data. This data key is then encrypted with your CMK. This approach enhances security by minimizing the exposure of the CMK.



6. Key Policies and IAM Roles:

   - KMS allows you to set key policies that define who can use the key and for what purposes. IAM (Identity and Access Management) roles are used to grant permissions to AWS services and users to interact with KMS and perform encryption and decryption operations.



7. Audit Trails:

   - KMS provides detailed audit trails through AWS CloudTrail. You can monitor key usage, track API calls, and receive logs of key-related events to ensure compliance and security.



8. Key Rotation:

   - KMS supports automatic key rotation, which enhances security by periodically replacing the cryptographic material of a key. This helps mitigate the risk associated with long-lived keys.



9. Multi-Region Replication:

   - KMS supports multi-region replication, allowing you to replicate your keys to different AWS regions. This is useful for disaster recovery scenarios and ensures that encrypted data can be accessed in the event of a regional failure.



10. Integration with Custom Applications:

    - KMS provides APIs that allow developers to integrate encryption capabilities into their custom applications, ensuring that data is encrypted before being stored or transmitted.



In summary, AWS Key Management Service (KMS) is a fully managed service that simplifies the process of encryption key management. It provides a secure and scalable solution for encrypting sensitive data, integrates seamlessly with various AWS services, and offers features such as key policies, audit trails, key rotation, and multi-region replication to enhance security and compliance.

AWS Certificate Manager (ACM) is a service provided by Amazon Web Services (AWS) that simplifies the process of procuring, managing, and deploying SSL/TLS certificates for secure communication over the internet. Here's an overview of ACM:



1. Certificate Provisioning:

   - ACM makes it easy to request and provision SSL/TLS certificates for use with AWS services and integrated applications. It supports wildcard certificates and Subject Alternative Name (SAN) certificates.



2. Integration with AWS Services:

   - ACM seamlessly integrates with several AWS services, including Amazon CloudFront, Elastic Load Balancing (ELB), API Gateway, and AWS Elastic Beanstalk. Certificates provisioned with ACM can be easily associated with these services to enable secure connections.



3. Managed Renewals:

   - ACM automates the renewal process of SSL/TLS certificates. It handles the complexity of certificate renewals, ensuring that certificates are automatically renewed before they expire, reducing the operational burden on users.



4. Free Public SSL/TLS Certificates:

   - ACM provides free SSL/TLS certificates for public use, allowing you to secure your website or web application without incurring additional costs for the certificates.



5. Private Certificate Authority (CA):

   - ACM Private CA is an extension of ACM that enables the creation and management of private certificate authorities. It allows you to issue private certificates for internal resources within your organization.



6. Secure Certificate Storage:

   - ACM securely stores and manages your certificates. This includes handling the private key securely and providing a secure storage solution for sensitive cryptographic material.



7. Automatic Deployment:

   - ACM supports the automatic deployment of certificates to integrated AWS services. Once a certificate is provisioned or renewed, ACM can automatically update associated resources with the latest certificate, simplifying the update process.



8. Domain Validation Options:

   - ACM offers multiple domain validation options, including email validation, DNS validation, and HTTP validation. These options ensure that the entity requesting a certificate has control over the domain for which the certificate is being issued.



9. Global and Regional Certificates:

   - ACM allows you to create certificates that are either regional or global. Regional certificates are specific to a single AWS region, while global certificates can be used across multiple regions.



10. Integration with AWS Identity and Access Management (IAM):

    - ACM integrates with AWS IAM, allowing you to control access to certificate resources and actions through IAM policies. This ensures that only authorized users or applications can manage certificates.



11. Visibility and Monitoring:

    - ACM provides visibility into certificate expiration dates and sends notifications when certificates are nearing expiration. This helps you proactively manage certificate renewals and avoid service interruptions.



In summary, AWS Certificate Manager is a service that simplifies the process of obtaining, deploying, and managing SSL/TLS certificates for secure communication over the internet. It automates certificate provisioning, renewal, and deployment, reducing the complexity of managing certificates and enhancing the security of your applications and services.


Distributed Denial of Service (DDoS) attacks are malicious attempts to disrupt the normal functioning of a targeted server, service, or network by overwhelming it with a flood of internet traffic. Amazon Web Services (AWS) provides services to mitigate and protect against DDoS attacks, and two key components are AWS WAF (Web Application Firewall) and AWS Shield. Here's an explanation of DDoS protection using WAF and Shield:



1. AWS WAF (Web Application Firewall):

   - Purpose: AWS WAF is designed to protect web applications from common web exploits and DDoS attacks.

   - Features:

     - Web Application Protection: WAF helps protect web applications by allowing you to define customizable rules that control which traffic is allowed or blocked.

     - Rules and Conditions: You can create rules based on conditions such as IP addresses, HTTP headers, HTTP body, or URI strings. This enables fine-grained control over the traffic that reaches your web applications.

     - Rate Limiting: WAF allows you to set rate limits to control the number of requests per minute that are allowed from a particular IP address or range, helping to mitigate DDoS attacks.

     - Integration with CloudFront and ALB: WAF integrates with Amazon CloudFront and Application Load Balancer (ALB), enabling you to deploy web application protection globally.



2. AWS Shield:

   - Purpose: AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS.

   - Features:

     - Global DDoS Protection: Shield provides global DDoS protection, automatically detecting and mitigating DDoS attacks at the edge locations.

     - Layer 3, 4, and 7 Protection: Shield protects against DDoS attacks at multiple layers, including network and application layers.

     - Advanced Threat Intelligence: AWS Shield leverages advanced threat intelligence to identify and mitigate DDoS attacks proactively.

     - Auto Scaling Integration: Shield integrates with Auto Scaling, allowing it to scale protection automatically based on traffic patterns.

     - Cost-Effective: AWS Shield provides cost-effective DDoS protection, and it is available in two tiers: Standard (automatically included at no extra cost for all AWS customers) and Advanced (providing additional features for more complex protection needs).



3. Integration:

   - AWS WAF and Shield can be used together for comprehensive DDoS protection. WAF focuses on protecting web applications by filtering and monitoring HTTP traffic, while Shield provides broader DDoS protection at the network and transport layers.



4. Operational Ease:

   - Both WAF and Shield are fully managed services, meaning AWS takes care of the operational aspects such as monitoring, detection, and mitigation of DDoS attacks. This allows users to focus on their applications rather than managing security infrastructure.



In summary, AWS WAF and Shield work in tandem to provide a robust defense against DDoS attacks. WAF offers web application protection with customizable rules, while Shield provides global DDoS protection at multiple layers. The combination of these services helps ensure the availability and security of applications hosted on AWS.

AWS Artifact Overview:



AWS Artifact is a service that provides on-demand access to AWS compliance reports and other documentation, helping customers and auditors understand how AWS services can be used to meet their compliance and regulatory requirements. Here's an overview of AWS Artifact:



1. Compliance Reports:

   - AWS Artifact offers a variety of compliance reports, including the AWS SOC (Service Organization Control) reports, PCI DSS (Payment Card Industry Data Security Standard) reports, and more. These reports are generated by independent third-party auditors and provide details about the security and compliance of AWS infrastructure.



2. Access to Documentation:

   - Users can access important compliance documentation, including attestation of compliance, agreements, and other artifacts required for regulatory purposes.



3. Download and Review:

   - AWS Artifact allows users to download compliance reports and other documents, enabling them to review and share them with auditors, regulators, or other stakeholders.



4. Agreements and Contractual Documents:

   - Users can access agreements, contracts, and other legal documents related to their use of AWS services. This includes the AWS customer agreement, data processing addendum, and business associate addendum for customers in regulated industries.



5. Security and Assurance:

   - AWS Artifact provides customers with the necessary information to assess the security and assurance of AWS services, facilitating their own compliance and risk management processes.



**AWS GuardDuty Overview:**



AWS GuardDuty is a managed threat detection service that continuously monitors for malicious activity and unauthorized behavior within AWS accounts. It uses machine learning, anomaly detection, and integrated threat intelligence to identify potential security threats. Here's an overview of AWS GuardDuty:



1. Continuous Monitoring:

   - GuardDuty performs continuous monitoring of AWS accounts, analyzing events, and network traffic for potential security threats.



2. Threat Detection:

   - Leveraging machine learning and threat intelligence, GuardDuty detects various types of threats, including unusual API calls, compromised instances, and potentially malicious activity.



3. Integration with AWS Services:

   - GuardDuty integrates with various AWS services, including CloudTrail, VPC Flow Logs, and DNS logs, to gather comprehensive data for threat detection.



4. Security Findings:

   - When GuardDuty identifies potential security threats, it generates security findings. These findings include details about the threat, affected resources, and recommended actions.



5. Prioritized Findings:

   - GuardDuty prioritizes findings based on severity, helping users focus on addressing the most critical security issues first.



6. Automated Responses:

   - Users can configure automated responses to specific GuardDuty findings, allowing for immediate action when potential threats are detected.



7. Centralized Management:

   - GuardDuty provides a centralized management console where users can view security findings across multiple AWS accounts, making it easier to manage security at scale.



8. Integration with AWS Security Services:

   - GuardDuty seamlessly integrates with other AWS security services such as AWS WAF and AWS Macie, providing a comprehensive security solution.



In summary, AWS Artifact is a service that provides compliance documentation and reports, while AWS GuardDuty is a threat detection service that continuously monitors AWS accounts for potential security threats. Both services contribute to the overall security and compliance posture of AWS users.

**AWS Config Overview: Enhancing Security Through Continuous Monitoring**



AWS Config is a robust service provided by Amazon Web Services (AWS) that plays a critical role in the continuous monitoring and assessment of the configuration settings within your AWS environment. Its primary objective is to ensure that your infrastructure remains secure and compliant by keeping track of configuration changes and evaluating settings against predefined rules.



**Continuous Monitoring of Resources:**



One of the key features of AWS Config is its ability to continuously monitor and record configuration changes across various AWS resources. By doing so, it creates a comprehensive inventory of resources, capturing essential details such as resource types, tags, and relationships. This continuous monitoring capability allows users to gain insights into resource changes over time.



**Configuration History and Change Tracking:**



AWS Config maintains a detailed configuration history, providing users with the ability to track changes to resources and understand who made those changes. This historical record is invaluable for auditing purposes, troubleshooting, and maintaining accountability within the AWS environment.



**Customizable Rules for Security Compliance:**



Users can define and enforce custom rules with AWS Config to ensure security and compliance. These rules evaluate the configurations of AWS resources against best practices and security standards. When a resource violates a rule, AWS Config generates a non-compliance report, allowing users to take corrective actions promptly.



**Configuration Snapshots and Comparisons:**



The service enables users to take configuration snapshots at specific points in time, facilitating comparisons to assess the impact of changes. This feature aids in identifying security vulnerabilities introduced by modifications to configurations, supporting proactive security measures.



**Integration with AWS CloudTrail:**



AWS Config seamlessly integrates with AWS CloudTrail, enhancing visibility into API activities. This integration enables users to gain a comprehensive understanding of who accessed resources, what changes were made, and when those changes occurred.



**Automated Remediation with AWS Config Rules:**



AWS Config Rules not only help identify non-compliance but also offer automated remediation. When non-compliance is detected, AWS Config Rules can trigger automated responses, enabling users to enforce security policies without manual intervention.



**Multi-Account and Multi-Region Support:**



AWS Config extends its monitoring capabilities across multiple AWS accounts and regions. This feature is particularly beneficial for organizations with complex cloud infrastructures, providing a unified view of configuration compliance.



**Security and Compliance Dashboards:**



AWS Config provides customizable dashboards to visualize configuration compliance and security status. These dashboards offer a consolidated view of rule evaluations, enabling quick identification of areas that require attention.



**Conclusion:**



In conclusion, AWS Config serves as a foundational service for maintaining a secure and compliant AWS environment. Its continuous monitoring, configuration history tracking, and customizable rules contribute to the proactive management of security. By leveraging AWS Config, organizations can enhance their overall security posture, ensuring that configurations align with best practices and compliance standards.

Amazon Security Hub Overview:



Amazon Security Hub is a service by Amazon Web Services (AWS) that consolidates and prioritizes security alerts and compliance status across AWS accounts. By aggregating findings from various AWS services and partner solutions, it offers a centralized dashboard for a comprehensive view of the security posture within an AWS environment.



Root User Privileges:



The root user in an AWS account possesses the highest level of access and control. It's crucial to manage root user privileges carefully due to their significant impact on security. To enhance security:



1. Avoid Using Root User for Everyday Tasks:

   It's advisable to refrain from using the root user for routine operational tasks. Instead, create and utilize AWS Identity and Access Management (IAM) users with the minimum necessary privileges for specific functions.



2. Multi-Factor Authentication (MFA):

   Enhance security by enabling Multi-Factor Authentication for the root user. MFA requires an additional authentication step, providing an extra layer of protection.



3. Delegate Responsibilities with IAM:

   Leverage IAM to create and manage users with specific permissions. Adhere to the principle of least privilege, ensuring that IAM users have only the permissions essential for their tasks.



4. Security Hub Findings and Monitoring:

   Regularly monitor Security Hub for security alerts, vulnerabilities, and compliance issues. This proactive approach facilitates the prompt identification and resolution of potential security threats.



5. Automated Remediation:

   Implement automated remediation workflows for Security Hub findings. This ensures timely responses to security issues, reducing the risk of manual errors or delayed interventions.



6. Regular Audits and Compliance Checks:

   Conduct periodic audits of IAM policies, Security Hub findings, and compliance checks to verify the effectiveness of security measures. This practice ensures alignment with industry best practices.



7. Incident Response Planning:

   Develop and regularly test an incident response plan that includes procedures for addressing security incidents identified through Security Hub and other monitoring tools.



Conclusion:



In conclusion, Amazon Security Hub serves as a centralized platform for managing security findings across AWS accounts. When dealing with root user privileges, it is essential to adopt best practices, including limiting root user usage, employing Multi-Factor Authentication, and delegating responsibilities through IAM. Regular monitoring, automated remediation, and incident response planning contribute to a robust security posture in AWS, allowing users to effectively leverage Security Hub while maintaining a secure and well-managed AWS environment.

Other services overview--

In the world of AWS, imagine containers as portable, self-sufficient boxes containing your applications and their dependencies. AWS ECS takes this concept further by employing Docker containers, ensuring uniformity across various environments. These containers are the building blocks for deploying applications consistently.



When we talk about ECS, we start with the concept of a "Task Definition." This is akin to a recipe for your application, detailing which containers to use, their configurations, and how they interact. Think of it as the blueprint guiding ECS on how to bring your application to life.



Once defined, these tasks become live instances of your application. They are the tangible entities, executing the instructions laid out in the task definition. A task can involve multiple containers, each contributing to the overall functionality, and they share the same set of resources.



Services in ECS play the role of caretakers for your tasks. They ensure that a specific number of tasks are always operational, automatically replacing any failed ones to maintain uninterrupted service. This orchestration of tasks happens within a "Cluster," which is a grouping of container instances.



These container instances are essentially the hosts for your tasks. Whether they are EC2 instances or Fargate instances, each comes with an ECS agent. This agent acts as a coordinator, communicating with ECS, launching and stopping tasks as necessary.



Speaking of coordination, ECS seamlessly integrates with other AWS services. From databases (RDS) to storage (S3) and identity management (IAM) to monitoring (CloudWatch), ECS collaborates with these services to provide a holistic environment for your containerized applications.



For managing incoming traffic and ensuring scalability, ECS integrates with the Elastic Load Balancer (ELB). This distributes traffic evenly across containers within a service, preventing overload on a single instance.



ECS offers flexibility in task scheduling. Whether you decide manually where your tasks should run or let ECS handle it automatically, the system aims to balance loads and resources efficiently. Additionally, task placement strategies help determine how tasks are distributed across instances.



One notable feature is the support for both managed and unmanaged launch types. With Fargate, AWS takes care of the underlying infrastructure, while with EC2 instances, you gain more control over the environment.



In summary, ECS simplifies the deployment and management of containerized applications, providing a structured approach to ensure they are where they need to be, doing what they're meant to do. From task recipes to the orchestrating agents and the hosting clusters, ECS facilitates a seamless and scalable containerized application environment.

Let's dive into Amazon EKS, a powerful service for managing and orchestrating containerized applications using Kubernetes. In this class, we'll break down the key components and concepts.



Kubernetes, often abbreviated as K8s, is an open-source container orchestration platform. EKS takes the robustness of Kubernetes and integrates it seamlessly into the AWS ecosystem.



At the core of EKS are "Clusters." These clusters act as the control plane for orchestrating containerized applications. They manage the deployment, scaling, and operation of application containers. Just like ECS, EKS clusters can scale to meet the needs of your applications.



Within these clusters, we have "Nodes." Nodes are the underlying EC2 instances or, with the introduction of AWS Fargate, serverless compute engine instances. Nodes are responsible for running the actual containers.



A critical concept in EKS is the "Pod." A Pod is the smallest deployable unit in Kubernetes, representing a single instance of a running process in a cluster. Pods can contain one or more containers, tightly coupled and sharing the same network namespace.



Now, let's talk about "Services." In Kubernetes, a Service is an abstraction that defines a logical set of Pods and a policy by which to access them. EKS integrates with Elastic Load Balancers to expose these services to the internet, ensuring high availability and load balancing.



"Deployments" in EKS allow you to declaratively manage the desired state of your application. You define how many replicas of a Pod should run, and Kubernetes ensures that the actual state matches the desired state.



For application configuration, Kubernetes introduces "ConfigMaps" and "Secrets." ConfigMaps store non-sensitive data in key-value pairs, while Secrets are used for confidential information like passwords or API keys.



EKS also provides seamless integration with other AWS services. Whether it's accessing databases, storage, or utilizing AWS Identity and Access Management (IAM), EKS allows for a cohesive ecosystem.



One significant advantage of EKS is its compatibility with Kubernetes tooling. This means you can leverage existing Kubernetes skills, configurations, and applications seamlessly.



As we explore EKS, remember that it is a managed Kubernetes service. AWS takes care of the undifferentiated heavy lifting, such as patching, updates, and maintaining the control plane, allowing you to focus more on building and running your applications.



In conclusion, Amazon EKS combines the power of Kubernetes with the ease of use and integration of AWS. It provides a robust and scalable platform for deploying, managing, and scaling containerized applications, making it a valuable tool in the world of cloud-native computing.

Let's delve into Amazon CloudFront, a content delivery network (CDN) service that accelerates the distribution of your web content. In this class, we'll explore its key components and how it optimizes the delivery of your resources to end-users.



At the heart of CloudFront is the concept of a "Distribution." A Distribution represents the collection of servers (edge locations) around the world that cache and deliver your content. These edge locations are strategically positioned to reduce latency and provide efficient content delivery.



When you set up a CloudFront Distribution, you designate the origin of your content. This can be an Amazon S3 bucket, an EC2 instance, an Elastic Load Balancer, or even a non-AWS origin server. CloudFront pulls the content from this origin and caches it at edge locations.



The term "Edge Locations" is crucial in understanding CloudFront's efficiency. These are data centers located in different geographic locations. When a user requests content, CloudFront delivers it from the nearest edge location, reducing latency and accelerating content delivery.



Cache behavior plays a significant role in determining how CloudFront interacts with the origin and serves content. You can set rules to specify when CloudFront should forward requests to the origin and when it should use the cached content from the edge location.



To secure your content and ensure that it is delivered over HTTPS, CloudFront provides easy integration with Amazon Certificate Manager (ACM) to manage SSL/TLS certificates.



CloudFront also supports "Distributions Types." Web Distributions are optimized for delivering dynamic and static web content, while RTMP Distributions are designed for streaming media files using the Adobe Real-Time Messaging Protocol (RTMP).



Logging and monitoring are crucial aspects of managing content delivery. CloudFront integrates with AWS CloudWatch for monitoring and logging to provide insights into the performance and behavior of your distribution.



One powerful feature of CloudFront is its ability to integrate with other AWS services seamlessly. Whether you're using it with S3 for scalable object storage, EC2 for compute resources, or AWS WAF for web application firewall protection, CloudFront enhances the overall performance and security of your applications.



In summary, Amazon CloudFront is a versatile CDN service that optimizes the delivery of web content by leveraging a global network of edge locations. With features such as easy integration, cache behavior customization, and robust security options, CloudFront becomes a valuable tool for enhancing the speed, reliability, and security of your web applications.

In this chapter, we will explore the nuanced process of deploying and managing machine learning models through Amazon SageMaker. SageMaker, a fully managed service, empowers developers and data scientists to build, train, and deploy machine learning models at scale. Our journey will encompass deploying models, managing endpoints, and automating the deployment pipeline.



Introduction

Before embarking on model deployment, it is imperative to ensure the readiness of model artifacts. We will discuss the intricacies of training models using SageMaker's built-in algorithms or custom algorithms, and how to save the trained model as artifacts in Amazon S3.



Model Deployment

Our exploration begins by understanding the various hosting environments SageMaker provides for deploying models. We will delve into real-time endpoints and batch transform jobs, dissecting the trade-offs between these environments based on factors such as latency and cost.



The chapter proceeds with a step-by-step guide on deploying models using SageMaker endpoints. This includes creating, configuring, and managing endpoints, with insights into considerations like instance types, scaling, and encryption.



Model Management

Effective model management is critical for long-term success. We will discuss best practices for versioning machine learning models in SageMaker, managing multiple model versions to facilitate A/B testing and rollback strategies.



Model monitoring and drift detection take center stage as we delve into the importance of ongoing monitoring for performance and drift. The chapter explores how to set up monitoring using SageMaker Model Monitoring and implement strategies to detect and address model drift.



Auto-scaling capabilities provided by SageMaker are explored, demonstrating how to dynamically adjust the number of instances serving predictions based on workload to optimize resource utilization and cost.



Continuous Deployment with SageMaker

The chapter concludes with a focus on continuous deployment. Strategies for building a deployment pipeline are discussed, emphasizing integration with AWS CodePipeline and AWS CodeBuild for automated model deployment.



Model testing and validation become crucial components of this pipeline, as we delve into best practices for automating testing processes to ensure that deployed models meet performance and accuracy requirements.



Advanced deployment strategies, including canary deployments and blue-green deployments, are explored to minimize downtime and risk when rolling out new model versions.



Conclusion

By the chapter's conclusion, readers will possess a comprehensive understanding of deploying and managing machine learning models with Amazon SageMaker. This knowledge equips data scientists, developers, and machine learning engineers to effectively utilize SageMaker for deploying robust and scalable machine learning solutions.

Amazon Sumerian is like a user-friendly tool from Amazon that helps people make cool virtual reality (VR) and augmented reality (AR) experiences without needing to be super techy. It's a web-based service, so you can create and show off your 3D scenes and apps without lots of complicated coding.



Imagine you want to make a virtual tour or a 3D game. With Sumerian, you use a Scene Editor to easily put things where you want them, like objects or characters. There's also a bunch of ready-made stuff you can use, making things quicker.



You can make your scenes come alive with movement and interaction using JavaScript or AWS Lambda, which is like adding smart behavior to your scenes. And the best part is, you don't have to worry about where your scenes are hosted - Sumerian takes care of that in the cloud.



Sumerian doesn't care what device you're using; it works on VR headsets, AR gadgets, or just regular web browsers. So, people can see and enjoy your creations on whatever they have.



Think about it like this: Sumerian is used for all sorts of things. People use it to make training simulations for jobs, like in healthcare or aviation. Businesses use it to make 3D displays for online shopping, making it more fun. Even in education, teachers use Sumerian to create cool and interactive lessons using VR and AR.



One more cool thing is that Sumerian can talk to other Amazon services, like one that turns text into speech or another that understands natural language. This makes your VR or AR experiences even more awesome!



So, in a nutshell, Amazon Sumerian is like a creative tool that makes it easy for anyone, even if you just finished college in India, to make really cool VR and AR stuff without getting lost in the tech jungle.

=========== Practical====

MFA-
first security is username,password
second security--MFA
1st time when we will put, download google authenticator 
from playstore. 1st time when it verifies user after that
authenticates only once.
root creates IAM user, and on every IAM user,
we can apply MFA.

aws.amazon.com--sign in-- root user--- search i am user--
--add mfa-- assign mfa-- device name: mobile_mfa --
-- authenticator app -- next-- show QR code--- scan from mobile
--- insert first code that u go on mobile--
---insert second code that u go on mobile-- add mfa
-- sign out--- sign in to the console---
root use sign in-- password-- mfa code

Creating vm for ubuntu OS --
1. give new name for virtual machine
2. select os(AMI)-- select ubuntu(free tier)
3. select instance type-- i.e. here we select configuration of machine--
so, selecting t2.micro-- we get ram 1 gb , 1 virtual cpu ,
nano is also free n=but ram is .5 gb 
small has more ram but it is chargeable
4. select hard disk or ssd -- till 30gb its free-- 
by default it gives 1 ssd

So sign in-- search ec2-- now on the left--click instances--
--launch instance--- instance name: ubuntu-machine--
scroll down---  os that i want is ubuntu-- select free tier machine--
-- instance type-- t2 micro-- key pair--create new key pair-- ubuntu-machine--
key pair type-- rsa---private key file format-- .pem ---
-- create key pair-- download this key pair-- we willnot
do anything in network security-- configure storage--
by default its of 8 gb u can maximize it to 30 gb--
by add new volume i can add new ssd but that will be chargeable--
-- number of instances--1-- now click on network settings--
in network info, u will get the code of the machine--
then there is firewall-- create security group--
allow ssh traffic from-- anywhere-- launch instance-- 
u will get the msg success-- click on the id in blue--
and ur ubuntu-machine is there--- in running stage--
now we get 2/2 becuase it creates 1 machine and 1 replica
in some other availabilty zone.
Now check select this machine-- and u will get all the
details of this machine-- owner is the root user and the number is the 
id of root user. securitty group starts with sg--
in networking 2 ids gets generated-- 1 is private 1 is public.
the client who has created this machine gets
public network addr and aws keeps with it private.
availability zone-- in region mubai this machine is
created in az ap-south-1b so replica will be in 1a.

Now accessing this m/c-- go up-- select machine-- connect
-- ssh client--

now how to acces this machine-- select ubuntu-machine.pem 
which u downloaded in ur system--- on the top in path panel--
type cmd-- cmd prompt will open--
> ssh -i ubuntu-machine.pem now got to the aws page their some
example is given copy from ubuntu@ec2-public addr of m/c 
in some zone--copy till end-- come to cmd prompt and paste--
so, > ssh -i - ubuntu-machine.pem ubuntu@-----everything copied--- 
ssh for accessing and -i for calling instance

r u sure to connect? yes

u will get ubuntu prompt--

$ clear--to clear the screean

to show the configuration of vm(ubuntu)--
$ cat /etc/os-release

to show the memory ram usage--(not of harddisk or ssd)
$free -m

to show no. of cpus--
$lscpu

to show how many ssd and their configuration--
$df -h

ip addr?
$ ip a

for exiting from ubuntu-- press x at top right

Now, second method to access ubuntu m/c--

at ssh client-- ec2 instance connect-- just scroll down--
connect. from here also we can connect our ubuntu machine.

but generally we use 1st method. 2nd method is used by owner becoz
he has link.
$exit-- u will be logged out

NOw come to ec2 instance-- deleting everything--
terminate everything -- goto ec2 dashboard-- refresh
and terminate all

Lightsail--for launching website. VPS- virtual private server
is most important. This is established by AWS backup center.
We can have our own setup with the help of VPS.
The unique things which comes in lightsail are--
1. vps
2. gives high speed ssd
3. DNS configuration
4. we can have our load balancer
we will launch website with the help of wordpress server
we can even configure SQL db.
now while configuring db, we notice that
m/c ip addr is not always constant.Instead we need
static ip addr. And for noone to see that ip addr, we
put firewall. for eg for connecting with python,
we use pymysql.connect and for host=ipaddr-- this addr is 
static. 
Light sail is also a compute service.
d/b ec2 and light sail is--
we can configure ec2,autoscale it,m/c take 5-7 mins to launch
while lightsail take-30-40 secs

login aws--
search lightsail--
now we cann see diff tabs--
instances, containers, databases,networking,storage,domains and dns, snapshots

in instance we can launch our website into webserver.
databases- we can configure dynamic website db.
networking- we can get static ip addr for our dynamic website.

create instance-- create an instance--
1. region--its mumbai --u can change region if its unavailable.
2. platform-- linux or windows--
so 1st seeing linux-- then select a blue print--app+os only--
selecting wordpress
if windows selected then--app+os-- no app for website launch
so, selecting linux-- app+os --wordpress
wordpress is certified by bitnami--
now, add launch script is for if u have any code to write
we dont need it--
enable automatic snapshots is for backup. its chargeable.
currently we dont need it. So keep it disabled.

instance plan-- $3.5,$5 etc.
These are free for 1st 3 months.

every region has different data transfer rate

identify ur instance--
instance name shoul be unique--lets say WordPress-1

create instance

now, u can see running m/c. its public ip addr is entioned ,
which we can be used to access this  instance.

Now copy this ip addr-- open new tab in browser--
ctrl+v-- enter--- 
click on sample page-- click hello world--
on the url bar , after ip addr , write /wp-admin

login page will come-- by default user name is user--
pwd is generated by cloud who gave wordpress service--
so come back to lightsail page-- click on the arrow next 
to 3 dots-- 1 cmd prompt window opens-- we will generate 
pwd here--
$ cat bitnami_application_password
we will get one password-- paste this password into user area.

login-- on the left users--click on it
 to change pwd.
post-- add new--add title-- lets say AWS Classes-- u can customize
left side + option -- click that-- u can select image/video/audio.. anything
ucan crop it etc.
on right -- click publish--it will give url-- copy and paste in new browser

logout

now we will delete everything--
click 3 dots on right-- delete

now coming on to tab-- databases-- 3 dots-- 
u can launch -- u can give user name --
if not given by default it will take-- i.e.
db_master_user

u can even give master db name.-- by default-- db_master


choose $15
data 
  
give database name which should be unique-- Database-2
-- create database

click 3 dots-- delete it.

Networking-- we can generate static ip addr 
for our database because everytime we will create our
workspace, ip addr will keep on changing, so,
-- then-- create static ip--

delete everything after done.

Next is RDS service---

Relatinal Database service works on structured data.
webserver<-->application server<-->database server
Database server-- it has OS and database. required
h/w dba tem, application server required s/w developer
AWS gives rds to manage all 3.
we can call this db in both linux and windows.
from windows to access rds we need- platform installer
and if from linux we need putty.
We can also put load balancer, to divert load.

So, in RDS we can get variety of databases, and apart from database server
we can also have machine.

Practical-- sign in aws

service-- rds-- click it
create database-- donot take restore multi AZ-- this is chargebale--
it creates database in the AZs of that particular region.

standard create--
engine option-- here we get options like--
Aurora MySQl,Aurora PostgreSQL,MySQL,MariaDB,PostgreSQL,
Oracle, Microsoft SQL Serer

take MySQL--
engine version-- keep by default selected
templates-- free tier
availabilty and durability
settings-- database-1
credential settings-- user name by defauklt-admin
give your password-
instance configuration- here it creates db server-burstable classes-- here it gives machine, put OS into that-and then db engine
by default t3 micro is selected-- it gives 2 virtual machines, 1 GB RAM, network-2085 Mbps

So in the dropdown select t2.micro with 1 vpc, 1 gb ram

storage-- free till 20 gb max can be 6144 gb

auto-scaling off by unclicking option

VPC it automatically creates

Then it is asking do u want access it publicly or privately?
choose public access

VPS security group-- we will create new security group.
VPC security group means firewall. 
so, group name= mysqlsecuritygrp

additional configuration- db port 3306

database authentication--pwwd authentication

monitoring- off--gives cpu utilization-- chargeable

database options--- initial db name-- db1

backup-- by default 7 days max 35 days -- disable

log exports-- cpu utlization, ram utilization, harddisk usage, etc-- disable

maintenance-- enable-- no preefrence

deletion protection- disable

--create database

db created

now select this db and click on its name.

click on the security group link--
to tell where we will access RDS-- in windows or linux
click on inbound rules-- edit inbound rules--add rule--
if its linux machine then security type is ssh, port 22
source- anywhere i.e. 0.0.0.0/0 ipv4
add rule-- for windows type- RDP- remote desktop--anywhere
if u donot want to create 2 differently then in security type
u can select all traffic--

-- save rule

now refresh -- we got 2 security gruops--
1 is by default.

Now on the left click on service and click on RDS again
select database-1 and refresh it.
let this db selected only

Now copy url on the top and paste it in new tab--

Now we r going to create our machine--
for linux and window.

click on service-- on search bar type ec2--
we will link security group of rds here. We are creating
this machine for database.
click instance-- launch instance- Linux-machine1
OS- amazon linux-- configuration frretier
instance- t2.micro
for accessing machine we have key pair-- create 
key-pair-- Linux-machine1--.pem-- create-- save it somewhere

network settings-- it creates vpc by itself--
now where we will create our machine-- so in our mumbai region
go to some az--subnet -1a

now i will not create a new security group-- i will connect it
with security group of rds security group-- so,
select existing security group-- in the drop down,
 u will see security group name--
select mysqlsecuritygroup

configure storage--

launch instance

click on th machine link--

now whenever u want to access linux machine with rds u will need s/w--
putty s/w and puttygen s/w.

Now before installing it close all the services. 
Acess we will see in next class

#-------------------------------------------
so closing services--
select linux-machine1- instance state-terminate
We access machine with the help of putty by using IPv4 public addr

on th left-- ec2 dashboard-- first delete key pair
select key pair-- action-- delete

we first remove rds to remove security group putted on my machine.

refresh ec2--
now in services put rds--
select database-1-- scroll down-- click VPC security groups--
scroll down-- edit inbound rules-- delete all the 3 securitites.
-- save rules-- refresh


now in services-- select rds-- data bases-- database-1 -- action-- delete
- delete me-- if give error -- then change final snapshot name-- database-4
delete
action-- delete


now the security is not yet deleted--  so goto instance dashboard--
select security group-action- delete

if its giving error because of networking then click on it--
click on the security-- delete it-- refresh

#--------------------------------------------------------------------------

Now we will see accessing-- so do the above setup--

Select the linux-machine1-- connect--

connect to instance-- ec2 instance connect-- see public ip addr-- copy this addr and keep it in notepad

now see in the aws dashboard--username by default is ec2-user
so keep this also in notepad

aws dashboard-- connect

now in linux , root user has all the permissions.

$ sudo su

# now i need to work on database m/c from this system--

install putty.exe file and puttygen.exe 

open puttygen s/w-- dialog box-- load-- load private key--
load the keygen file-- save private key--yes--mylinuxmachine.ppk

open--putty--dialog box- host name-- put public ip addr copied in notebook
-- ssh-- on the left pane click on +ssh-- auth-- credential-- browse--
open the above converted .ppk file-- open-- connect once--

login as: now we can work on sql from here.
so, login as- ec2-user
so goto admin who is root user in linux -- sudo su 
dev (device) is the directory on which we install s/ws
first install mysql-- 
$yum install -y mysql
$clear
#now here, mysql -u root, instead we will write 
mysql -h we will give security key of rds instance also called as endpoint.
so go to aws dashboard rds->databases-->click on database-1-- scroll down -- there is endpoint and port-- copy that and paste after mysql -h , then give space and write -u and give username. In our case its admin.
space -p (i.e. it will ask for pwd), then
give space and give db name. In our case it is db1.

eg. 
$ mysql -h database-1.caq4zioryhxr.ap-south-1.rds.amazonaws.com -u admin -p db1

write this on the prompt.
Give pwd.

and we will get mysql prompt.

mysql>


Now s3 glacier--storage efs,ebs,s3,storage gateway,glacier.

s3 has 3 types of storage-- frequently everyday, infrequently in few months, glacier yearly.

prac-- s3 ke saath machine ko mount karne ke liye we use gateway, lly, for glacier we need to install some s/w called fastglacier.

download fastglacier -- install it.

aws glacier service is very secure, durable, cost is min, its a cloud storage platform.

sign in to aws-- service-- s3 glacier--
-- its all chargeable-- one page open where we create space-- its called vaults.
create vault-- see region (mumbai region)
-- name MyBackupData--
then notification is chargeable-- so off
--create vault
this vault if deleted, gets deleted in 24 hrs.
click on MyBackupData-- and see what got created.
Search for IAM service-- IAM-- on the left see option users--
add user-- parinita-1 -- tick provide
user access to aws mgmt console-- this helps in 
giving pwd. This pwd is given by companies man user.
then- are u providing console access to a person?
select I want to create an IAM user.

console pwd-- autogenerated or custom? select custom
Give the pwd.
then, keep ticked users must create a new pwd at next sign in 
next

set permissions--i donot want to add into any user-- so select attach policy directly--
tiick administrator access
or u can search administrator full access
both are same

scroll down -- next-- create user

it gives me csv file containg user id and pwd

Now come back to user list--
tick this user-- refresh-- now it has not got 
active key age yet. And what I want is , this 
user has permission to keep file on glacier--
so we are supposed to this user's access key and this user's secret key we will give to fastglacier.
and we will put from our m/c directly from the tool . So we will attach fastglacier with 
this user. 

install and open fastglacier-- window opens--
on left-- accounts-- 1st time we do add new account, next time we can do manage account--
Account name- ParinitaJain
account type- AmazonGlacier {this gets created in the selected region
Access Key ID-- minimize this window and 
come back to aws dashboard-- we are in users-- we have user ParinitaJain who has all the permissions of admin.
click on the user-- we can see the details regarding this user,
see security credentials-- scroll down there is access key-- create access key--
(as the access key gets created, secret key also gets created.)--
access key best practices and alternatives--
select first option-- command line interface-- scroll down-- tick I understand
the above recommendation and want to procedd
to create access key-- next-- create access key.

now we got both access key and secret access key.

now goto fastglacier-- paste access key id--
now goto aws and copy secret access key--
and paste in fastglacier-- add new account

now aws is mounted

our vault is in mumbai region
click on  + asia pacific (mumbai)-- 
we can see our vault--MyBackupData-- select that and upload files.

now if we want to keep file here then-- 
if u directly want to upload or u can create folder and keep the docs.
u can even upload folder directly.

if u want to download data-- then select file-- 1. expected,2. standard, 3,. bulk

for fast download--1st option

the charge is there according to option.

now goto aws vault-- view detail--on the pricing page 

if u want to edit data retrieval policies--
then-- data retrieval policies--
no retrieval limit, free tier only, specify a max retrieval rate - any-- save changes

Now to delete--
from fastglacier delete file--
then right click on the vault from fastglacier only-- delete vault
-- it is not deleting instantly because it takes almost 24 hrs to delete.

So what u can do is-- on the top left
on fastglacier--ther is account Parinitajain 
under Accounts--manage accounts--
select ParinitaJain-- delete that--
or to delete vault from aws-- goto services--amazon glacier--s3 glacier-
tick MyBackupData--delete--


Now got to IAM user from services-- tick the user
put the name of the user --delete it.


Cloud watch-- It works as a monitoring tool. It is a dashboard and works as a counting tool.
Here we can see--
1. No. of services running i.e. EC2
2. No. of storage i.e. S3
3. Load Balancer. If load balancer is there than that is connected to how many AZs of that particular region.
4. SES
5. CPU% utilization i.e. EC2 and S3 utilization.

all this data is shown on the dashboard of cloudwatch. 
We can also see the graph. Many different formats are there like number format, stack format, And also mail the user this data in pdf format but
this is chargeable.

This detail is shown after every 5 mins.
Cloud watch is also called as basic monitoring.
EC2 gives CPU utilization.  U can do coding here--
if cpu utilization>=70% then create alarm
based on which u can create some action.
So for alarming , we can create email with the help of SNS-- and autoscaling can be done.
Here we can create lambda function also, i.e. 
if servers are unnecessary free, then set a trigger to close them. 
with the help of lambda , we can also create sns service
Instead of sns, u can also use ansible service. In ansible service, we define playbook.
Here we do coding of the same i.e. whether
to increase the vertical scaling or horizontal scaling. The playbook will run.
U can even write python script for autoscaling.

If every 5 mins monitoring is done, then it is called as basic monitoring. And there is no
charge for basic monitoring.
If u want for every 1 min, then pay.

We will do lambda coding 

__Elastic Load Balancer-- ELB distributes traffic comin on the load balancer.
It helps in reducing fault tolerance and helps in getting high scalability.

load balancers are of 3 types--
1. Application lb, --2016
2. network lb --2018-2019
3. Classic lb-- this is also known as previous geenration. Its a conmbination of both above.

Application lb- It is used for specific application , i.e. if app for laptop , then on laptop, if for computers then computers. It works on http/https

https - require ssl certification

Network lb-- latency less, efficient n/w

OSI 4th layer- transport

protocol-- tcp/udp/tls

Prac--
1. create 2 ec2 instnces-- windown2012 with both servers in different availability zones.

but VPC of both instance will be common. That is region will be same.

2. Now on both web servers we will install 
web server IIS, Internet Information Service. 
For running php website we install IIS/tomcat and for java we install apache or tomcat.

3. then we will create html page on both server ec2 instance.

4. create load balancer.

5. then we will create target group and register both instances in this target group.

6. then the dns of load balanacer got created. , copy its dns and paste on
any browser, then run.

so we have user(gives url)-- elb(dns)-- we will keep security
of these 3 machines same , so that it can connect with it.


Prac--


service--ec2--name server1_1a--windows 2012 b2 base-- t2,micro--key pair-- server1_1a .pem-- create-- save

network settings-- subnet-- select ap-south1a--

then come to firewall-- we will create security group-- becauze both the machines will have
common vpc. 
So, security group name-- LBSecuritygrp

now, come to inbound security groups rules-- 
type will be rdp because windows machine is there, source type- anywhere

Then add security group-- type-- htttp becoz we will deal with website. source type anywhere

then add security group-- type-- https,
security group anywhere -- this we are creating 
so that lb can balance the load.

http and https for website and , rdp for windows

configure storage--  for windows we need 30 gb

launch instance--

now 2nd machine--

launch instance-- server2-1b--windows2012--
instance type-- t2.micro-- key pair-- dropdown--server1_1a (1st machine key pair-- user id pwd for both machines will be differerent)

network settings-- edit--since 1st machine is in 1a,
we will create this m/c in 1b.
firewall-- select existing security group--
so, in common security groups dropdown-- LBSecuritygrp

-- launch instance

now we will access this machine 1 by 1.
 tick 1st machine-- goto connect--
rdp client-- whenever we want to access 
windows m/c, we need RDF-- download rdp extension file.

user id is Administrator and for password-- click get pwd -- it will ask for upload private key-- Server1.pem-- decrypt-- copy this pwd and keep it in notepad
and u can mail this to client.

Now double click the downloaded rdp file,--
connect-- put pwd -- ok--yes

Now 2nd m/c has same key value but pwd is diffrent.

Now we will put IIS.
So goto Server Manager(downloaded on ur m/c)--this is used for hosting--

add roles and features-- next-- role based or feature based installation--next--
scroll down-- webserverIIS--next - next-
next-next- install

Now this same iis server we will install at other server machine-- close

Now in this insatlled 2012 window machine-- c drive-- because we have installed web server-- we can see initpub folder because of the server installed.

click initpub-- there is a folder wwwroot.
whatever website u have-- paste that inside this folder.
i.e. wherever we r hosting website on the webserver machine we need to install iis.

iis-85 and iisstart delete them from wwwroot.
 these r system files which we donot need.
Now right click inside-- new file-- index.html-- open it-- Server Availabilty zone 1 a Parinita Jian-- save as- name index.html all files


Now minimize it and do same thing in another machine--
 Select -- connect-- rdp client-- doenload RDFile-- save it-- get password-- upload pkfile-- i.e. Server1_1a-- different pwd we get-- decrypt pwd =-= copy in notebook.

open rdp file of srver2 m/c-- give name pwd-- yes-- server manager--(in rdp we used this for accessing web portal of mysql)-next-- do same setting--install

c drive -- initpub-- wwwroot-- delete 2 files--
new text doc-- index.html-- Server 2 AZ 1b --
save as type all files.


Now we will connect this m/c with load balancer.

aws dashboard--services-- ec2 dashboard-- load balancer-- on the left there is a load balancing-- inside which ther is load balancer-- create load bbalancer--
we will take app lb-- http-- create--
Name of lb-- ApplicationLB
Scheme-- Internet Facing
IP add type-- IPv4 , dual stack gives ipv4 and ipv6. Since we have same m/c take ipv4 only.
In case of diff m/cs take ipv6.

LB protocol--HTTP port 80

AZs-- here we tell whether we want to distribute load in diff m/cs. VPC of both will be common---select 1a and 1b- since our m/c is in 1a abd 1b.
note-we donot have our m/c in 1c.

Next-- next configure security group-- take security group of ec2 instance only i.e select existing security group-- i.e. LBSecurityGroup-- next

Target group-- --now register both the azs in this -- name-- TG1-- target type is our machine and machine is called instance- 
HTTp-80-- scroll down--advanced health checkups-- scrooll down--
register target -- tick 1st one-- add to registerd-- now untick 1st one and tick 2nd one-- add to registerd-- next-- create- close

observe--state is provisioning-- az is 1a 1b

now on the left click target group -- we can see TG1-- 

now testing if load is getting distribbuted-- so we need to use lb DNS-- 
so from left pane go to load balancer-- click on the name of lb-- click on the box--scroll down 
and see description-- scroll down -- we can't see dns-- copy it-- now open a browser--
paste the dns-- enter-- open another browser lets say internet explorer-- copy the dns --
goon refreshing-- and u will see 1a, 1b 1a 1b

Now exiting server-- the iis that we installed close in both

now goto target group-- tg1-- action-- delete
if its in use-- goto lb-- action delete 

ec2 instance-- 1st server-- instance--terminate-- 2nd instance-- delete

keypair-- delete

sg-- select--delete

ec2 dashboard--- security group--deleyte--
if its ther-- scroll down-- inbound rule--
select any 1-- edit inbound rule-- delete all from here

referesh ec2 dashboard-- now delete security group

___Autoscaling--

create ec2 instance with name MyWindowsAutoScaling
download .pem key-pair with same name.
then cretae 
its AMI image, then launch configuration - MyLaunchConfiguration

Now we have not told yet , after how cpu utilization , we will create new server
or how much server we will launch.

So,  now after configuration, come to autoscaling frm
left and  click autoacaling groups. 
scroll down, create autoscaling group-- 
name-- MyAutoScalingGroup

scroll down, Launch template-- here we will choose our 
configuration -- so click switch to launch configuration--
scroll down-- now u will get ur launch configuration name--
i.e. MyLaunchConfiguration

-- next

VPC - do nothing--

AZs and subnets-- if we will not
 select anything, then by default, as selected by us,
instance will be created in AZ 1a,
but in case if this goes down then noone will be able to 
access the data.
So click all the AZs here.

-- next

load balancing-- no load balancer--

health check-- to check if server is working properly.
by default it pings for 5 times after 5min,
if no response received then considers dead.

-- change it to 60 secs-- i.e. after every 1 min it will 
check if instance is active or not.

--- next

desired-- 2 --as the ec2 instance starts and run, 2 ec2 instances will be launched
minimum-- 1 --if traffic is less, then keep atleast 1 server.
maximum-- max 4-- if traffic increases.

scaling policies--
target tracking autoscaling

scroll down--
metric type-- avg cpu utilization--
target value--10 --
instances need -- 60
this means , if in 1 min , avg cpu utilization
is 10% or more than that then increase instance

-- next

Add notifications--  nothing--
tag-- nothing--

--next

-- create auto scaling

select MyAutoScalinggroup and refresh-- 

now on the left-- ec2 dashboard--
instances(running)-3 i.e by default 1+desired 2
for this keypair and snapshot is 1.
now clcik on instances-- u can see 3 servers running

m/c ban ke taiyaar hai, zarurat padne par mil jaegi.

Now accessing machine that we created-- click that
(just for understanding we r terminating other machines)

so tick 1st machine-- connect-- rdp client-- 
user name-- administrator
get pwd-- upload pk file-- upload MyWindowsAutoscaling.pem-- decrypt--
got the pwd-- copy it and save in notepad
now in all the m/cs user name will be administrator only and this pwd will
be same.

Download RemoteDesktop file--
click it-- connect it--  give pwd -- ok -- yes

now, it will ask do u want to access n/w-- yes--

if u want to change pwd-- search computer management
-- local users and groups-- double click users--
right click adminstrators-- set pwd-- proceed--
give pwd and remember that.

now we will work on this windows--
there is something known as batch file--

batch file is .bat file-- contains bootable files.whenever or system starts,
we need .bat file which r stored in ROM.
this run by default and setup system.

now creating batch file--
on the desktop-- right click-- new text doc--
double click and open file-- write a.bat -- this is the name of the file
and save this as a.bat--all files

now double click and run this file to increase the load
on the cpu.
goon clicking this file--
now minimize it
come to aws dashboard-- ec2 instance--
refresh it-- u can see machine running
now select any of the launched machine-- click connect--
--rdp client-- rdfile download-- donot do get password.
user name and pwd will be same --
double click-- give pwd-- we got another m/c.

now create batch file-- right click-- new-- a.bat--
save as a.bat--all files

double click a.bat file-- and open many many files

aws dashboard-- ec2 instance-- refresh---
instances launched.
now close the windows-- refresh
now termination--- instance state-- terminate all --

refresh page-- 
delete autoscaling grp -- delete-- refresh it will take little time.

remove keypair
on the left Images-- AMIs-- delete 
remove snapshots
remove security groups--
Images-- 


aws BUDGET--
services-- aws budget-- create budget-- use a template
-- zero spend budget-- name-- let it default
email recepents- parinita@itvedant.com -- u can give any preexisting email

-- create

--next--

--save

--delete

AWS route S3-- DNS- domain namesystem
--it works on tcp port np. 53--
What is dns-- whatever is the websites domain name or url,
convert that into IP addr. 
This ip addr is stored in application server-- which is stored inside
web server-- which is acessed by user by using url.
so when we search www.facebook.com -- we are not actually accessing 
this name but the ip addr behind this name. 53 route does this connection.

this is chargeable--
prac-- aws route53--
DNS mgmt-- whenever we host our website on url,
the dns connects it to the server ip addr .

Availability monitoring-- monitors cpu utilization,
resource utilization, health checkup, autoscaling ,etc

first, by godaddy we purchase space for getting web url,
instead u can use domain registration by using route53.
-- choose a domain name-- .com, .net evey extension has charges.

Next is VPC- cirtual private/public cloud--
its like cloud inside cloud where we can launch our service.
For accessing these services person needs access
and authentication.

On the left-- VPC-- 1 Route Tables is by default-- 
this is the owner-- inside it other clouds gets created.

SNS--Simple Notification Service. It is a PUSH based system.
Its of 2 types-- Push based service-- SNS, easy to setup and operate and send notifications
from cloud. Can publish msg.We can send msgs to emails/mobile SMS/lambda/http/https etc,
delivers msgs immediately to subscribers or subscribed application,
It can send msgs to all popular devices like android,google,apple,etc

Pull based service-- SQS service. 

prac of sns--
aws dashboard--
search-- SNS-- pub-publish , sub-subscribe
so fisrt we create topic-- next step--
FIFO, standard--
Fifo guarantees sending msg in fifo order.
2nd is standard-- all the msgs will be sent guaranteed
but not in the order.
choose standard--

name-- Notification_service1
-- create topic-- now our service is created
now first we need to subscription--
create subscription now-- topic arn will be created by default--
-- select that-- we need to subscribe
rule we will write like if cpu utilization>70%,
or resource utilization>70% then do autoscaling

in protocol u can choose-- email
endpoint is ur emailid-- parinita@itvedant.com

-- create subscription
now open ur mail id-- spam-- subscription confirmation.

refresh page--

now come to topics-- select the topic- notification-service1-- publish msg
ARN we will get--
subject-- CPU utilization--
come to msg body custom payload for each delivery protocol and write--
Content : - any msg u can put-- create 

message attr-- type- string , name - put some name
value-- put anything like-- Itvedant Andheri-- Publish msg

--SQS--
Simple Queue Service-- It is a Pull based service.
msgs produced are putted into SQS msg queue which is
also a container and stored there--
for 30 secs it hide .
SQS visibility timeout-- for 30 sec-- max visibility timeout can be 
12 hrs. but msgs can be kept in queues from
1 min to 14 days and default retention
period is 4 days.

the msg isnot lost in SQS.

-----Storage------  EBS,EFS,S3,Glacier
Elastic block storage, Elastic File System,
Simple Storage Service, Glacier

One is object level, next is block level

EBS- Block level storage
Now when we create ec2- we 1st put OS-- and then we 
connect storage-- SSD and here we choose EBS--

In windows, the root dirve is C drive-- here we put OS
In linux root drive is /
In aws , as we logged in, ec2-- select machine--
windows-- now here if u notice-- microsoft windows server 2022 base-- root device type-- ebs is written.
this ebs is 30 gb in case of windows and 8 gb for linux and ubunut..

Now my OS gets installed in this EBS only.

If the m/c is created in np-south1-a then storage
should also be in np-south-1-a
Now, if storage is not available in 1a,
then storage will not be given in 1b and 1c.
Or instead , we can write lambda func to get storage 
in 1b 1c.
or we can create efs-- we get 2 instances, in same or diff AZ.
7 types of strorage devices are-- ipo-2 -- then I can connect
My storage with multiple azs.
If in diff azs, then u can use router, or route s3, for route53 u will
need virtual device which is chargeable.

In EBS we mount extra 5 gb or as required on 30 gb,
and it will be charged.

while in efs-- extra charge is not taken even if 
it is mounted until some data is kept. And the charge is according
to data size kept on EFS.

Now there are many kinds of storage--

pract-- aws dashboard
services--ec2-- left elastic block storage-- volumes--
create volume-- vol type-- 7 types of ssd are there--
general purpose ssd(gp2),
general purpose ssd(gp3)
provisioned IOPS ssd(io1)
provisioned IOPS ssd(io2)
cold hdd(sc1)
throughput optimized HDD(st1)
Magnetic(standard)

1 vm storage in diff AZ-- then provisioned IOPS SSD(io2)
provisional io2 for vm with multiple io

so in dashboard--vol settings--general purpose ssd gp2
size- 5 gb

-- create vol

now select and refresh--

now creating another storage in 1b--
ebs--volumes--gp2--5 az-1b-- create vol

now select both and refresh.
Give the name to volumes-- Storage 1-a, Storage 1-b

now goto vm--creting mc--
ec2- instance-- launch instance--
name--server_ubuntu-- os--ubuntu--
free tier-- instance type- t2.micro-- keu pair-- server_ubuntu
--rsa -- .pem-- create key pair--
download
n/w setting-- edit-- subnet-- means which az--?--1a
firewall-- create security group--put launch-wizard-1
inbound sg rules-- type-- ssh
source type-- anywhere

configure storage--
8 gb gp2-- this is root vol, here OS installs.
-- add new vol-- 6 gb gp2--

-- add new vol

-- launch instance

now select server_ubuntu and refresh
Now goto ebs from left. our 2 storages 
are in 1a and 1b.

in use is charged.
It creates snapshots of machines remeber 
to dele them.
Now the ebs storage in 1-a rename it to pendrive.

now come to ec2 instance--
select server_ubuntu connect-- we get cmd prompt
we got public ip addr--

lsblk means list block-- check attached vol in follwing machine
$clear
$lsblk
see the o/p--
xvda is 8gb -- os is installed here
xvdb is some pendrive of 6 gb

now come to ec2 dashboard--and attaching 1b storage 
to machine--
elastic block storage volumes-- tick 1b
ation-- attach volume--
instance dropdown is blank-- 
(if u want to connect then router or gateway khareed ke vpc instance will come here.)
Now deselect 1 b and select 1a.

1a--action-- attach volume-- instance-- select the instance--

if instances are in diff azs, then use iop2 and thats chargeable.

in linux main folder is /dev-- attach vol-- the moment it
connects-- it is charged.

now checking from m/c put -- lsblk
we can see xvdf 5gb

now detaching--
volumes-- tick pendrive-- action- detach--refresh

$lsblk-- the attached vol is gone.

now how to use data in attached 5gb --
here only root user can work-- 
$sudo su
or
$sudo!!

before keeping any file its a rule to format it.
$mkfs.ext4 /dev/xvdf

now we will create directory inside it.
$mkdir /test
$cd /
$ls
this cmd we have written in m/c and so in m/cs ssd
dir will get created.
Now, i want to mount or store this test folder
into pendrive--
so to mount or to copy test folder from root machine to pendrive(5g)--
cmd is--
$mount /dev/xvdf /test/
$mountpoint /test
$cd /test/
$touch 1 2
$echo "Welcome to itvedant">demo.txt
$ls
$cat demo.txt
$cat 1
$do lsblk

now to remove -- for unmounting /test-- cmd is 
$cd /
$umount /test/
$cd /test/
ls
cd /

$lsblk

Now detaching from aws dashboard-- first detach then delete

volumes-- storage 1a---  action--detach--delete
storage 1 b-- action--detach-delete
ubuntu ssd


check if any snapshots are there-- if there then delete them
now come to instances--terminate instance.
remove keypair--delete keypair
del security group

refresh and see if sg is deleted or not.

Lambda function--
scale up, scale down

steps-- create s3 bucket, create IAM role, create lambda function, add trigger,test

aws services-- now s3 bucket is global we can access it
 through router53 or gateway.
craet bucket-- lambda-dummy-triggerred --scrooll down - create bucket
Now creating IAM role-- for adding permissions to lambda func--
Search IAM-- Click IAM dashboard- on left roles-- create role--
usecase-- lambda-- next
permission attach-- search-- awsLambdaBasicExecutionRole-- select that--next
(as our file gets uploaded in s3 bucket, we get all the de
tails like
how many files got uploaded, and all the other details.)
next it will ask role name-- lambda_trigger_role
description u can give anything-- scroll down it gave in code--
giving tag is not imp. 
key- name

value is optional

next -- creating lambda function-- services-- lambda
charged only when code is running

on the left we can see aws dashboard-- functions--
create functions
author from scratch
func_name-- lambda_trigger
runtime--python3.9
authenticate--x86_64
permissions-- change default execution role--
here the role that we have created is attached

so, choose-- use existing role--
existing role-- drop down-- lambda_trigger_role--
advanced setting is chargeable
--create func
4th step -- now adding trigger--
lambda->functions>lambda_trigger--add triggger-- s3
our case is-- as the file gets uploaded in s3 bucket , 
generate trigger which we can see in cloudwatch.

bucket-- pre created bucket name will be there-select that
event type-- All object create events
prefix--optional
suffix - optional

recursive invocation-- tick that to acknowledge
-- add

trigger succefully addded.

now u scroll down-- u can see code--

now in this coding-- 
def lambda_handler(event,context):
  print(event)-------add this line

now goto file and save
now come back to s3 bucket--
services-search-s3 bucket--
click on the previous bucket-- upload -- add file--
now upload some file-- uploaded successfully.

nowwe want to see if our trigger triggered--
so services- lambda function--click lambda _func-- monitor--
view cloudwatchlogs--

come to monitor-- detail--test

now deleting--

lambda functions-->tick--->delete
then iam role-- lambda_trigger_role-- delete
deleting s3 bucket--permanently delete
















 











































jan 24,jan24,
activate aws-jan 26
ec2-part1,2-feb 4
mfa-feb5
ec2 create ubuntu- feb 5
autoscaling-feb 9
s3 glacier-feb21
lightsail-feb25
rds-part1-feb 25
rds prac-26
cloud watch-feb 27
eload balancer- feb 27
budget setting-mar 1
sns service- mar 1
sqs service- mar 1
aws budget- mar 2
aws cloud form-mar 2
rds prac- mar 4
storage EBS theory-mar 4
EBS prac-mar 5
lambda func- mar 8
load balancer- mar 10
SMOTE - mar 10


Cloud formation--
Without cloud formation the challenges that used to 
come are-- if a project comes then the XYZ company needs,
 instance,VPC,Autoscaling,Load Balancer

Cloud formation creates templates -- we write all the demands
so we go to aws console-- cloud formation tab-- and we will load
template there. and so we can automate.
then if we want to de-provicion or delete,
we can delete this stack.










 






